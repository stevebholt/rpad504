[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "RPAD 504 - Data, Models, and Decisions",
    "section": "",
    "text": "This course introduces computer-based tools for planning, policy analysis, and decision making. Topics include evaluating the quality of data for decision making, database construction and information management, administrative and policy models in datasets, making decisions with multiple criteria, an introduction to probability, and the use of simulation models as testbeds for policy making. Emphasis is placed on summarizing information meaningfully for policy makers and different stakeholders and using replicatable workflows in common statistical software packages."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "syllabus504.html",
    "href": "syllabus504.html",
    "title": "Syllabus",
    "section": "",
    "text": "üë®‚Äçüè´: Stephen Holt, Ph.D.\nüìÖ: Wednesdays from 6:00 pm to 8:50 pm in Husted 204\nüìß: sbholt@albany.edu\nüè´: Tuesdays and Thursdays from 1:00 pm to 5:00 pm or by appointment - book here; in-person in Milne 324 or online in GatherTown.1\n‚òéÔ∏è: 518-442-3309\n\n\n\nThis course introduces computer-based tools for planning, policy analysis, and decision making. Topics include evaluating the quality of data for decision making, database construction and information management, administrative and policy models in datasets, making decisions with multiple criteria, an introduction to probability, and the use of simulation models as testbeds for policy making. Emphasis is placed on summarizing information meaningfully for policy makers and different stakeholders and using replicatable workflows in common statistical software packages.\nBy the end of this course, students should be able to 1) critically analyze administrative problems and evaluate potential solutions; 2) consider questions of accountability and responsiveness in administrative action; and 3) analyze the potential challenges in implementing public programs through both public and private organizations. Students should leave class with a deep understanding of both the technical challenges and competing values present in implementing and managing public programs.2\n\n\n\nThere is no required text for the class.\nAdditional readings will be made available on Zotero in the class library.\n\n\n\nStata18/IC - The student version of Stata is sufficient for the course. The software costs $48 for a 6 month license (sufficient for this course) and purchase information can be found here.\n\n\n\n\n\n\nImportant\n\n\n\nWhen you order Stata, the company will send an email with a link to the download and a pdf with the license information you will need to enter after installing the software to activate it. Please do this before the second week of class. Typically, they provide this within a few hours of the order, but it‚Äôs worth accounting for the possibility of delays.\n\n\nStata is a statistical software that will provide an opportunity for you to learn the basic logic and intuition using code to conduct data analysis. The coding language for Stata is very simple, clear, and straight forward, so it serves as a good entry point to getting comfortable using programming languages for analyzing data and learning more about the world around us. Once you adjust to the basics of analysis with an easy and intuitive coding language like Stata, transitioning to other software languages (e.g., R, Python in industry; SAS, SQL in government) will be much less onerous.\nZotero - Zotero is a free platform for organizing and sharing academic work and it is how readings will be distributed to the class. You can download the app here. I suggest also downloading the browser connector for whatever browser you use - it helps save a dramatic amount of time when conducting a literature review. During the first week of classes you will receive an invitation to join the class Zotero library. If you do not receive one, please email the professor. I provide an introduction to Zotero and the features most applicable to this class in the embedded video below.\n\n\n\n\n\n\nNote\n\n\n\nThere are a variety of citation organizer software options out there for researchers to use and, having experimented with most of them, Zotero is by far the best. It is free; Open Source; designed and developed by researchers; and governed as a non-profit to protect the integrity and accessibility of the software.\n\n\n\n\n\n\n\n\n\n\nTip on Zotero\n\n\n\nWhen you receive an announcement that the group invitations for the class library have been sent, go to the Zotero.org website, login to your account, and click the groups tab. You should see RPAD 504 with a red button that says join. Click join. In the Zotero app, click the green refresh circle in the top right and the class library should appear.\n\n\n\n\n\nEach week, our in-person meetings will involve a mix of lecture and application of class concepts in lab work. Often class time will be split between these two activities evenly, but some sessions will lean deeper into one or the other based on the material. As group work gears up, I will allocate some time in the back end of class time for working with groups on the final report assignment. At the end of class, attendance will be taken each week and that will factor directly into students‚Äô participation grade.\n\n\n\n\n\n\nA Note on Groups\n\n\n\nIn this class you tackle one (1) broader research project on a team. This is by design because most professional work is conducted on teams, most decisions in public work are made by consensus, and opportunities to learn to adjust to different working styles while managing projects helps develop important and transferable skills. Once groups are assigned, I strongly encourage you to meet with your groups with an early video call and get to know each other, your schedules, and so on. Communication helps dramatically in successful groups. That said, some group dynamics can - in very rare circumstances - become bad for all involved. I provide two means for coping: first, students can use peer evaluations to ensure accountability in group work. And second, if that is insufficient, the group can vote to remove a member and inform me of the decision. No harm, no foul. This should be considered a measure of last resort and only pursued if a group member has made no contributions to the group.\n\n\n\n\n\n\n\nThe following assignments will form the basis of your grade in this course:\n\nFinal Report (35 points)\nWeekly Problem Sets (30 points)\nMid-term Exam (15 points)\nFinal Exam (15 points)\nParticipation (5 points)\n\n\n\n\n\n\n\nImportant\n\n\n\nAll assignments will be turned in via Brightspace.\n\n\n\n\n\nTo assess your understanding of the course material and your ability to apply concepts to real policy issues, you will be working in groups to prepare a 10 page report that i) describes a social problem with policy implications or an emerging policy issue, ii) uses data to describe the problem statistically, and iii) makes an informed recommendation on a policy change to improve the conditions that define the problem. The primary datasets used in the examples and problem sets throughout the course will be the datasets you will draw upon to construct your report. We will go over an example the second week of class to introduce you to the mix of art and science that goes into crafting good policy reports using data. You will also prepare a presentation for these results to deliver at the end of the semester (see Section¬†1.7.1).\nAcceptable sources. Generally, your research should use primary sources more than secondary sources. Primary sources include, but are not limited to: government reports, legislative hearings and testimonies, court decisions, government auditor reports, and rigorous academic research. Secondary sources are summaries and interpretations of primary sources. Secondary sources include, but are not limited to, articles from major newspapers and news magazines and more conceptual or theoretical academic research. Blogs and Wikipedia are not acceptable sources (though both can be good ways to get a broad sense of a topic before digging deeper). Be an intelligent consumer of information by evaluating secondary sources for potential political bias. If it is well known that a particular source is liberal or conservative, you must compensate for this in the paper. Acknowledge its bias and balance the information with something from a source on the other side of the political spectrum. Here are some places to start with your research, but feel free to consult other sources:\n\nAcademic research published in public administration, political science, economics, or policy journals such as: Nature Human Behaviour, Public Administration Review, Journal of Public Administration Research and Theory, Journal of Public Policy Analysis and Management, American Political Science Review, American Journal of Political Science, Journal of Politics, American Economic Review, Journal of Labor Economics, Journal of Human Resources, Quarterly Journal of Economics, Policy Studies Journal.\nCongressional testimony (available through Lexis/Nexis via the library) and reports completed by Congressional committee staff\nAgency Inspector General reports\nCongressional Research Service reports\nU.S. Government Accountability Office reports\nThe library also has research support services for this class, which can be found here\n\nSubmission of papers. The final report will be due by midnight on the designated due date (details to be distributed early in the semester). Electronic versions of the paper are to be submitted via the SafeAssign link in Brightspace. It is the student‚Äôs responsibility to ensure the electronic file is readable and not corrupted. Please note: once you hit the submit button in SafeAssign, you cannot go back and submit a different version.\nLate assignments. An assignment is considered late if the paper copy is not submitted at the beginning of class, if the electronic file is not submitted before class, and/or if the electronic file is not readable. Assignments (electronic or hard copy) submitted 10 minutes after the beginning of class or the launch of the following module will be considered late and will be automatically reduced by 10%. Papers submitted one day after the due date will be automatically reduced by 20%; essays submitted two days after the assigned date will be reduced by 30%, etc. Exceptions will be made for documented extreme health and family emergencies.\n\n\n\nFor 10 weeks throughout the semester (see Section¬†1.7.1 for the specific weeks), problem sets will be posted to Brightspace and the course site by the end of the day Thursday. They will be due before the start of class the following week, leaving both weekend and weekday stretches available for you to complete the problem sets. Problem sets will involve applying concepts and activities we complete in class together to new but similar datasets and problems on your own. You will be expected to turn in 1) a clean answer sheet that provides the final solution to any problem; and 2) a .log file that shows the code used to derive the solutions to the problems.\n\n\n\n\n\n\nWarning\n\n\n\nThe problem set will be cumulative in the sense that each week will add new concepts and issues for you to tackle that will often build on skills used the previous week. This carries two important implications. First, you will want to be careful not to miss any problem sets, as they provide important opportunities to confirm your progress in the class and understanding of the skills we are learning and missing problem sets might compound any struggles you are having. Second, each week will likely have opportunities to practice skills learned in prior weeks en route to practicing new material.\n\n\nEach problem set is worth 3 points.\n\n\n\nThere will be two in-class exams over the course of the semester. The week before the exam, study aids will be distributed and the exams will be taken in-class. Details about the exams will be provided ahead of the scheduled exams.\n\n\n\nAttendance and participation in class discussion and activities will determine the participation points you earn at the end of the semester.\n\n\n\n\n\n\nKey:\n\n\n\n\n\n\n\nSymbol\nMeaning\n\n\n\n\nüë®‚Äçüè´:\nLecture\n\n\n‚úÖ:\nProblem Set Assigned (due before the next class)\n\n\nüë•:\nLab\n\n\nüìÖ:\nExam or Final Report Due\n\n\nüìÑ:\nReading is from an article/chapter on Zotero (by author last name)\n\n\n\n\n\n\n\n\n\n\n\n\nDate\nTopic\nAssignments\nReadings\n\n\n\n\n8/28\nIntro to course\n\n\n\n\n9/4\nData Origins\nüë®‚Äçüè´\nüìÑ Knox et al.¬† üìÑ Llolbrera & Hall  üìÑ Holt & Vinopal\n\n\n9/11\nAssessing Data Quality\nüë®‚Äçüè´\nüìÑ National Academies of SEM:  Appendices A and C\n\n\n9/18\nSummarizing Data I\nüë®‚Äçüè´  ‚úÖ  üë•\nüìÑ Huntington-Klein\n\n\n9/25\nSummarizing Data II\nüë®‚Äçüè´  ‚úÖ  üë•\n\n\n\n10/2\nData Management I\nüë®‚Äçüè´  ‚úÖ  üë•\nüìÑ Kroenke & Auer (Ch. 1 & 2)\n\n\n10/9\nData Management II\nüë®‚Äçüè´  ‚úÖ  üë•\nüìÑ Kohler & Kreuter\n\n\n10/16\nMidterm Exam\nüìÖ\n\n\n\n10/23\nGeographic Information and Mapping Data\nüë®‚Äçüè´  ‚úÖ  üë•\n\n\n\n10/30\nWriting About Data\nüë®‚Äçüè´  ‚úÖ  üë•\n\n\n\n10/30\nLight Statistics Introduction\nüë®‚Äçüè´  ‚úÖ  üë•\n\n\n\n11/6\nStatistical Models I\nüë®‚Äçüè´  ‚úÖ  üë•\n\n\n\n11/13\nStatistical Models II\nüë®‚Äçüè´  ‚úÖ  üë•\n\n\n\n11/20\nPresentations and Wrap\nüë®‚Äçüè´  ‚úÖ üìÖ\n\n\n\n11/27\nThanksgiving Break\n\n\n\n\n12/4\nFinal Exam\nüìÖ\n\n\n\n\n\n\n\n\n\nPublic policy is a professional field; therefore, I emphasize professional skills in the classroom and assignments. Professional skills are punctuality, adhering to deadlines, and preparedness.\nAfter the first week, readings for each week should be completed by the Tuesday of that week (that is, BEFORE CLASS!). While much of the class reading comes from assigned readings, I will cover other material in my lectures. You will be responsible for knowing this material too!\nA large body of well-designed research has demonstrated the detrimental effects of laptops on learning in a lecture/discussion based environment. Please be courteous and do not use your computers for anything other than class related work (taking notes and so on). Cellular phones are not to be used during class time!\nLetters of recommendation. If you are a hard working student and serious about a career in public service, I will be a dedicated advocate for you on the job market and will happily write letters of recommendation on your behalf. There is, however, one condition and one recommendation. The condition: I will not write a letter of recommendation for your while you are in my class. This is because to write a good faith, sincere, and thoughtful recommendation, I will need to be able to consider your work as a whole, and while the class is on-going, my assessment of you will be incomplete. After the semester is over, I am happy to help in any way I can, including writing letters. The recommendation: Make an appointment to visit my office hours at least once over the course of the semester to talk informally about your goals, career interests, and other professional ambitions so I can get a better sense of who you are as individuals. The better I know you, the more effective I can be at writing letters on your behalf and thinking of you when opportunities arise.\nAttend/participate in class! Again, class participation is 5% of your grade, and you can‚Äôt participate if you‚Äôre not in class or watching the class videos. If you DO need to miss class (emergencies, sickness, etc.), please contact me as soon as possible and let me know. It will be your responsibility to notify me and to get any notes/materials from other students.\nCell phones: we all have them, and they can be quite distracting. I ask that you please be courteous and silence your cell phone and leave it out of sight (in a pocket/purse/bag) during class.\nFeel free to eat and drink in class. I only ask that you do so quietly and in a manner that does not disrupt class.\nAll assignments and non-textbook readings will be posted to the class Zotero Library. I will email any announcements or updates to the class and also post them in the Blackboard. Report any trouble accessing anything on the Blackboard as soon as you encounter the problem.\nI have a strict open door policy. If there is anything about the course, the assignments, the grading, the material, class, or anything related to public administration/policy or statistics broadly that you would like to discuss, do not hesitate to visit me during office hours or email me. I can respond via email, schedule a phone call, or schedule a separate meeting. I am here to help, so please do not hesitate to reach out to me. (But please be respectful of my time!)\nHAVE FUN! Public administration/policy is a broad topic that explores big, important questions that affect everyone. Discussing these topics should be as fun and interesting as it is challenging.\nThe table below lays out the grading scale that will be used in assigning final course grades.\nStudents with special physical and/or learning needs will be accommodated. Please notify the Disabilities Office and me as soon as possible so that reasonable accommodations can be made.\n\n\n\n\n\n\n\nWarning\n\n\n\nThroughout the semester, I may add or subtract readings as needed to adjust the course according to your progress, engagement, and interests.\n\n\nTable. Grade Scale Used for Calculating Class Grades\n\n\n\nPercent\nGrade\nPoints\n\n\n\n\n93-100\nA\n4.0\n\n\n90-92\nA-\n3.7\n\n\n87-89\nB+\n3.3\n\n\n83-86\nB\n3.0\n\n\n80-82\nB-\n2.7\n\n\n77-79\nC+\n2.3\n\n\n73-76\nC\n2.0\n\n\n70-72\nC-\n1.7\n\n\n67-69\nD+\n1.3\n\n\n63-66\nD\n1.0\n\n\n60-62\nD-\n0.7\n\n\n&lt; 60\nF\n0.0\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe percent refers to the percent of available weighted points earned. Each assignment is weighted by the proportion of the final grade made up by the assignment itself, as described above.\n\n\n\n\nAcademic honesty is something your professor takes very seriously. Cheating in any form will not be tolerated. Students are required to be familiar with the university‚Äôs academic honesty policies; ignorance is not an excuse for dishonest behavior. In all cases of cheating, a Violation of Academic Integrity Report will be submitted to the Dean of Graduate Studies to be placed in your university file, with copies provided to you, the department head, and the Dean of Rockefeller College. Additional penalties may include some combination of the following: revision and re-submission of the assignment, reduction of the grade or failure of the assignment, reduction of the course grade or failure of the course, filing of a case with the Office of Conflict Resolution and Civic Responsibility, suspension, or expulsion. For a more detailed description of the university‚Äôs academic honesty policies, visit the site.\n\n\n\n\n\n\nChatGPT and Other LLMs\n\n\n\nBy now, we are all aware of the technological advances in generative large language models (LLMs) trained on large quantities of written language scraped from around the internet. The University policy considers the use of ChatGPT and other generative LLMs to produce classwork without explicit permission from the instructor an act of plagiarism. I do not permit the use of ChatGPT or other generative LLMs in this course. First, generative LLMs can at times invent fictional sources, recombine information that is confidently stated but ultimately incorrect, and can produce generally mediocre and formulaic writing. Such events make the output unreliable - particularly for people aiming to be professionals working in the institutions that govern our society. Second, and more importantly, grappling with complicated trade-offs, collecting and synthesizing complex information thoughtfully, and going through the process of articulating your decisions and the knowledge base that inform them is a large part of an effective professional career. Learning in general is an arduous process that involves practice, trial and error, and confronting your current limits before finding ways to overcome them. In short, learning is work and the process by which that work occurs is often reading and writing, poorly at first and much better over time. I do think that, properly understood, LLMs can be a useful tool in managing routine tasks in which you have mastered the background, can detect and correct errors, and can use such tools effectively. However, early in your careers and in your academic lives, the very purpose of being in a graduate program is to have opportunities to learn new things (or old things in new ways) and using an LLM to do the work involved in learning will only cheat you of opportunities to learn, grow, and develop deeper and more lasting skills. Finally, and more practically, if you are caught using LLMs to produce the work assigned in this class, the work will be given a 0 and you will be cited for plagiarism.\n\n\n\n\n\nWe are committed to providing an accessible learning environment for all students. This includes students with physical, sensory, medical, cognitive, learning, mental health, and other disabilities. If you have, or think you may have a disability, please contact Disability Access and Inclusion Student Services (DAISS) by emailing daiss@albany.edu or calling 518 -442-5501. DAISS staff will explain the documentation and registration process, and set you up with an appointment. Once you have completed registration, you will be provided with a letter to inform your instructors that you are a student with a disability registered with DAISS, and which lists the recommended reasonable accommodations for your courses.\n\n\n\nThe Counseling Center (518-442-5800; 400 Patroon Creek Blvd, Suite 104) offers counseling and consultations regarding personal concerns, self-help information, and connections to off-campus resources. More information can be found at their site.\n\n\n\nSUNY-Albany offers a great collection available in several different media. Access to research help and library tutorials can be found online at the library‚Äôs site.\nFor information about SUNY-Albany‚Äôs Dewey Graduate Library, which is located on the Downtown Campus, visit their site.\n\n\n\nThe university offers a number of services for students who need assistance with writing and research projects. Support is available in the Writing Center (518-442-4061; 140 HU) and at the University Library. Information about the Writing Center can be found at their site.\n\n\n\nTitle IX of the Education Amendments of 1972 is a federal civil rights law that prohibits discrimination on the basis of sex in federally funded education programs and activities. The SUNY-wide Sexual Violence Prevention and Response Policies prohibit offenses defined as sexual harassment, sexual assault, intimate partner violence (dating or domestic violence), sexual exploitation, and stalking. The SUNY-wide Sexual Violence Prevention and Response Policies apply to the entire University at Albany community, including students, faculty, and staff of all gender identities. The University at Albany provides a variety of resources for support and advocacy to assist individuals who have experienced sexual offenses.\nConfidential support and guidance can be found through the Counseling Center (518-442-5800, or online), the University Health Center (518-442-5454, or online), and the Interfaith Center (518-489-8573, or online). Individuals at these locations will not report crimes to law enforcement or university officials without permission, except for in extreme circumstances, such as a health and/or safety emergency. Additionally, the Advocates at the University at Albany‚Äôs Advocacy Center for Sexual Violence are available to assist students without sharing information that could identify them (518-442-CARE, or online).\nSexual offenses can be reported non-confidentially to the Title IX Coordinator within The Office for Equity and Compliance (518-442-3800, or online, Building 25, Room 117) and/or the University Police Department (518-442-3131, or online).\n\n\n\n\n\n\nImportant\n\n\n\nPLEASE NOTE: Faculty members are considered ‚Äúresponsible employees‚Äù at the University at Albany, meaning that they are required to report all known relevant details about a complaint of sexual violence to the University‚Äôs Title IX Coordinator, including names of anyone involved or present, date, time, and location.\n\n\nIn case of an emergency, please call 911.\n\n\n\nA tentative grade given only when the student has nearly completed the course but due to circumstances beyond the student‚Äôs control the work is not completed on schedule. The date for the completion of the work is specified by the instructor. The date stipulated will not be later than one month before the end of the session following that in which the Incomplete is received. The grade I is automatically changed to E or U unless work is completed as agreed between the student and the instructor.\n\n\n\nStudents are excused, without penalty, to be absent because of religious beliefs, and will be provided equivalent opportunities for make-up examinations, study, or work requirements missed because of such absences. Students should notify the instructor of record in a timely manner, and the instructor will work directly with students to accommodate religious observances. Online courses will not schedule any assignment deadlines on religious holidays.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus504.html#rpad-504",
    "href": "syllabus504.html#rpad-504",
    "title": "Syllabus",
    "section": "",
    "text": "üë®‚Äçüè´: Stephen Holt, Ph.D.\nüìÖ: Wednesdays from 6:00 pm to 8:50 pm in Husted 204\nüìß: sbholt@albany.edu\nüè´: Tuesdays and Thursdays from 1:00 pm to 5:00 pm or by appointment - book here; in-person in Milne 324 or online in GatherTown.1\n‚òéÔ∏è: 518-442-3309",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus504.html#course-description",
    "href": "syllabus504.html#course-description",
    "title": "Syllabus",
    "section": "",
    "text": "This course introduces computer-based tools for planning, policy analysis, and decision making. Topics include evaluating the quality of data for decision making, database construction and information management, administrative and policy models in datasets, making decisions with multiple criteria, an introduction to probability, and the use of simulation models as testbeds for policy making. Emphasis is placed on summarizing information meaningfully for policy makers and different stakeholders and using replicatable workflows in common statistical software packages.\nBy the end of this course, students should be able to 1) critically analyze administrative problems and evaluate potential solutions; 2) consider questions of accountability and responsiveness in administrative action; and 3) analyze the potential challenges in implementing public programs through both public and private organizations. Students should leave class with a deep understanding of both the technical challenges and competing values present in implementing and managing public programs.2",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus504.html#required-text",
    "href": "syllabus504.html#required-text",
    "title": "Syllabus",
    "section": "",
    "text": "There is no required text for the class.\nAdditional readings will be made available on Zotero in the class library.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus504.html#required-software",
    "href": "syllabus504.html#required-software",
    "title": "Syllabus",
    "section": "",
    "text": "Stata18/IC - The student version of Stata is sufficient for the course. The software costs $48 for a 6 month license (sufficient for this course) and purchase information can be found here.\n\n\n\n\n\n\nImportant\n\n\n\nWhen you order Stata, the company will send an email with a link to the download and a pdf with the license information you will need to enter after installing the software to activate it. Please do this before the second week of class. Typically, they provide this within a few hours of the order, but it‚Äôs worth accounting for the possibility of delays.\n\n\nStata is a statistical software that will provide an opportunity for you to learn the basic logic and intuition using code to conduct data analysis. The coding language for Stata is very simple, clear, and straight forward, so it serves as a good entry point to getting comfortable using programming languages for analyzing data and learning more about the world around us. Once you adjust to the basics of analysis with an easy and intuitive coding language like Stata, transitioning to other software languages (e.g., R, Python in industry; SAS, SQL in government) will be much less onerous.\nZotero - Zotero is a free platform for organizing and sharing academic work and it is how readings will be distributed to the class. You can download the app here. I suggest also downloading the browser connector for whatever browser you use - it helps save a dramatic amount of time when conducting a literature review. During the first week of classes you will receive an invitation to join the class Zotero library. If you do not receive one, please email the professor. I provide an introduction to Zotero and the features most applicable to this class in the embedded video below.\n\n\n\n\n\n\nNote\n\n\n\nThere are a variety of citation organizer software options out there for researchers to use and, having experimented with most of them, Zotero is by far the best. It is free; Open Source; designed and developed by researchers; and governed as a non-profit to protect the integrity and accessibility of the software.\n\n\n\n\n\n\n\n\n\n\nTip on Zotero\n\n\n\nWhen you receive an announcement that the group invitations for the class library have been sent, go to the Zotero.org website, login to your account, and click the groups tab. You should see RPAD 504 with a red button that says join. Click join. In the Zotero app, click the green refresh circle in the top right and the class library should appear.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus504.html#structure-of-the-class",
    "href": "syllabus504.html#structure-of-the-class",
    "title": "Syllabus",
    "section": "",
    "text": "Each week, our in-person meetings will involve a mix of lecture and application of class concepts in lab work. Often class time will be split between these two activities evenly, but some sessions will lean deeper into one or the other based on the material. As group work gears up, I will allocate some time in the back end of class time for working with groups on the final report assignment. At the end of class, attendance will be taken each week and that will factor directly into students‚Äô participation grade.\n\n\n\n\n\n\nA Note on Groups\n\n\n\nIn this class you tackle one (1) broader research project on a team. This is by design because most professional work is conducted on teams, most decisions in public work are made by consensus, and opportunities to learn to adjust to different working styles while managing projects helps develop important and transferable skills. Once groups are assigned, I strongly encourage you to meet with your groups with an early video call and get to know each other, your schedules, and so on. Communication helps dramatically in successful groups. That said, some group dynamics can - in very rare circumstances - become bad for all involved. I provide two means for coping: first, students can use peer evaluations to ensure accountability in group work. And second, if that is insufficient, the group can vote to remove a member and inform me of the decision. No harm, no foul. This should be considered a measure of last resort and only pursued if a group member has made no contributions to the group.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus504.html#assignments",
    "href": "syllabus504.html#assignments",
    "title": "Syllabus",
    "section": "",
    "text": "The following assignments will form the basis of your grade in this course:\n\nFinal Report (35 points)\nWeekly Problem Sets (30 points)\nMid-term Exam (15 points)\nFinal Exam (15 points)\nParticipation (5 points)\n\n\n\n\n\n\n\nImportant\n\n\n\nAll assignments will be turned in via Brightspace.\n\n\n\n\n\nTo assess your understanding of the course material and your ability to apply concepts to real policy issues, you will be working in groups to prepare a 10 page report that i) describes a social problem with policy implications or an emerging policy issue, ii) uses data to describe the problem statistically, and iii) makes an informed recommendation on a policy change to improve the conditions that define the problem. The primary datasets used in the examples and problem sets throughout the course will be the datasets you will draw upon to construct your report. We will go over an example the second week of class to introduce you to the mix of art and science that goes into crafting good policy reports using data. You will also prepare a presentation for these results to deliver at the end of the semester (see Section¬†1.7.1).\nAcceptable sources. Generally, your research should use primary sources more than secondary sources. Primary sources include, but are not limited to: government reports, legislative hearings and testimonies, court decisions, government auditor reports, and rigorous academic research. Secondary sources are summaries and interpretations of primary sources. Secondary sources include, but are not limited to, articles from major newspapers and news magazines and more conceptual or theoretical academic research. Blogs and Wikipedia are not acceptable sources (though both can be good ways to get a broad sense of a topic before digging deeper). Be an intelligent consumer of information by evaluating secondary sources for potential political bias. If it is well known that a particular source is liberal or conservative, you must compensate for this in the paper. Acknowledge its bias and balance the information with something from a source on the other side of the political spectrum. Here are some places to start with your research, but feel free to consult other sources:\n\nAcademic research published in public administration, political science, economics, or policy journals such as: Nature Human Behaviour, Public Administration Review, Journal of Public Administration Research and Theory, Journal of Public Policy Analysis and Management, American Political Science Review, American Journal of Political Science, Journal of Politics, American Economic Review, Journal of Labor Economics, Journal of Human Resources, Quarterly Journal of Economics, Policy Studies Journal.\nCongressional testimony (available through Lexis/Nexis via the library) and reports completed by Congressional committee staff\nAgency Inspector General reports\nCongressional Research Service reports\nU.S. Government Accountability Office reports\nThe library also has research support services for this class, which can be found here\n\nSubmission of papers. The final report will be due by midnight on the designated due date (details to be distributed early in the semester). Electronic versions of the paper are to be submitted via the SafeAssign link in Brightspace. It is the student‚Äôs responsibility to ensure the electronic file is readable and not corrupted. Please note: once you hit the submit button in SafeAssign, you cannot go back and submit a different version.\nLate assignments. An assignment is considered late if the paper copy is not submitted at the beginning of class, if the electronic file is not submitted before class, and/or if the electronic file is not readable. Assignments (electronic or hard copy) submitted 10 minutes after the beginning of class or the launch of the following module will be considered late and will be automatically reduced by 10%. Papers submitted one day after the due date will be automatically reduced by 20%; essays submitted two days after the assigned date will be reduced by 30%, etc. Exceptions will be made for documented extreme health and family emergencies.\n\n\n\nFor 10 weeks throughout the semester (see Section¬†1.7.1 for the specific weeks), problem sets will be posted to Brightspace and the course site by the end of the day Thursday. They will be due before the start of class the following week, leaving both weekend and weekday stretches available for you to complete the problem sets. Problem sets will involve applying concepts and activities we complete in class together to new but similar datasets and problems on your own. You will be expected to turn in 1) a clean answer sheet that provides the final solution to any problem; and 2) a .log file that shows the code used to derive the solutions to the problems.\n\n\n\n\n\n\nWarning\n\n\n\nThe problem set will be cumulative in the sense that each week will add new concepts and issues for you to tackle that will often build on skills used the previous week. This carries two important implications. First, you will want to be careful not to miss any problem sets, as they provide important opportunities to confirm your progress in the class and understanding of the skills we are learning and missing problem sets might compound any struggles you are having. Second, each week will likely have opportunities to practice skills learned in prior weeks en route to practicing new material.\n\n\nEach problem set is worth 3 points.\n\n\n\nThere will be two in-class exams over the course of the semester. The week before the exam, study aids will be distributed and the exams will be taken in-class. Details about the exams will be provided ahead of the scheduled exams.\n\n\n\nAttendance and participation in class discussion and activities will determine the participation points you earn at the end of the semester.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus504.html#class-schedule",
    "href": "syllabus504.html#class-schedule",
    "title": "Syllabus",
    "section": "",
    "text": "Key:\n\n\n\n\n\n\n\nSymbol\nMeaning\n\n\n\n\nüë®‚Äçüè´:\nLecture\n\n\n‚úÖ:\nProblem Set Assigned (due before the next class)\n\n\nüë•:\nLab\n\n\nüìÖ:\nExam or Final Report Due\n\n\nüìÑ:\nReading is from an article/chapter on Zotero (by author last name)\n\n\n\n\n\n\n\n\n\n\n\n\nDate\nTopic\nAssignments\nReadings\n\n\n\n\n8/28\nIntro to course\n\n\n\n\n9/4\nData Origins\nüë®‚Äçüè´\nüìÑ Knox et al.¬† üìÑ Llolbrera & Hall  üìÑ Holt & Vinopal\n\n\n9/11\nAssessing Data Quality\nüë®‚Äçüè´\nüìÑ National Academies of SEM:  Appendices A and C\n\n\n9/18\nSummarizing Data I\nüë®‚Äçüè´  ‚úÖ  üë•\nüìÑ Huntington-Klein\n\n\n9/25\nSummarizing Data II\nüë®‚Äçüè´  ‚úÖ  üë•\n\n\n\n10/2\nData Management I\nüë®‚Äçüè´  ‚úÖ  üë•\nüìÑ Kroenke & Auer (Ch. 1 & 2)\n\n\n10/9\nData Management II\nüë®‚Äçüè´  ‚úÖ  üë•\nüìÑ Kohler & Kreuter\n\n\n10/16\nMidterm Exam\nüìÖ\n\n\n\n10/23\nGeographic Information and Mapping Data\nüë®‚Äçüè´  ‚úÖ  üë•\n\n\n\n10/30\nWriting About Data\nüë®‚Äçüè´  ‚úÖ  üë•\n\n\n\n10/30\nLight Statistics Introduction\nüë®‚Äçüè´  ‚úÖ  üë•\n\n\n\n11/6\nStatistical Models I\nüë®‚Äçüè´  ‚úÖ  üë•\n\n\n\n11/13\nStatistical Models II\nüë®‚Äçüè´  ‚úÖ  üë•\n\n\n\n11/20\nPresentations and Wrap\nüë®‚Äçüè´  ‚úÖ üìÖ\n\n\n\n11/27\nThanksgiving Break\n\n\n\n\n12/4\nFinal Exam\nüìÖ",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus504.html#class-policies",
    "href": "syllabus504.html#class-policies",
    "title": "Syllabus",
    "section": "",
    "text": "Public policy is a professional field; therefore, I emphasize professional skills in the classroom and assignments. Professional skills are punctuality, adhering to deadlines, and preparedness.\nAfter the first week, readings for each week should be completed by the Tuesday of that week (that is, BEFORE CLASS!). While much of the class reading comes from assigned readings, I will cover other material in my lectures. You will be responsible for knowing this material too!\nA large body of well-designed research has demonstrated the detrimental effects of laptops on learning in a lecture/discussion based environment. Please be courteous and do not use your computers for anything other than class related work (taking notes and so on). Cellular phones are not to be used during class time!\nLetters of recommendation. If you are a hard working student and serious about a career in public service, I will be a dedicated advocate for you on the job market and will happily write letters of recommendation on your behalf. There is, however, one condition and one recommendation. The condition: I will not write a letter of recommendation for your while you are in my class. This is because to write a good faith, sincere, and thoughtful recommendation, I will need to be able to consider your work as a whole, and while the class is on-going, my assessment of you will be incomplete. After the semester is over, I am happy to help in any way I can, including writing letters. The recommendation: Make an appointment to visit my office hours at least once over the course of the semester to talk informally about your goals, career interests, and other professional ambitions so I can get a better sense of who you are as individuals. The better I know you, the more effective I can be at writing letters on your behalf and thinking of you when opportunities arise.\nAttend/participate in class! Again, class participation is 5% of your grade, and you can‚Äôt participate if you‚Äôre not in class or watching the class videos. If you DO need to miss class (emergencies, sickness, etc.), please contact me as soon as possible and let me know. It will be your responsibility to notify me and to get any notes/materials from other students.\nCell phones: we all have them, and they can be quite distracting. I ask that you please be courteous and silence your cell phone and leave it out of sight (in a pocket/purse/bag) during class.\nFeel free to eat and drink in class. I only ask that you do so quietly and in a manner that does not disrupt class.\nAll assignments and non-textbook readings will be posted to the class Zotero Library. I will email any announcements or updates to the class and also post them in the Blackboard. Report any trouble accessing anything on the Blackboard as soon as you encounter the problem.\nI have a strict open door policy. If there is anything about the course, the assignments, the grading, the material, class, or anything related to public administration/policy or statistics broadly that you would like to discuss, do not hesitate to visit me during office hours or email me. I can respond via email, schedule a phone call, or schedule a separate meeting. I am here to help, so please do not hesitate to reach out to me. (But please be respectful of my time!)\nHAVE FUN! Public administration/policy is a broad topic that explores big, important questions that affect everyone. Discussing these topics should be as fun and interesting as it is challenging.\nThe table below lays out the grading scale that will be used in assigning final course grades.\nStudents with special physical and/or learning needs will be accommodated. Please notify the Disabilities Office and me as soon as possible so that reasonable accommodations can be made.\n\n\n\n\n\n\n\nWarning\n\n\n\nThroughout the semester, I may add or subtract readings as needed to adjust the course according to your progress, engagement, and interests.\n\n\nTable. Grade Scale Used for Calculating Class Grades\n\n\n\nPercent\nGrade\nPoints\n\n\n\n\n93-100\nA\n4.0\n\n\n90-92\nA-\n3.7\n\n\n87-89\nB+\n3.3\n\n\n83-86\nB\n3.0\n\n\n80-82\nB-\n2.7\n\n\n77-79\nC+\n2.3\n\n\n73-76\nC\n2.0\n\n\n70-72\nC-\n1.7\n\n\n67-69\nD+\n1.3\n\n\n63-66\nD\n1.0\n\n\n60-62\nD-\n0.7\n\n\n&lt; 60\nF\n0.0\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe percent refers to the percent of available weighted points earned. Each assignment is weighted by the proportion of the final grade made up by the assignment itself, as described above.\n\n\n\n\nAcademic honesty is something your professor takes very seriously. Cheating in any form will not be tolerated. Students are required to be familiar with the university‚Äôs academic honesty policies; ignorance is not an excuse for dishonest behavior. In all cases of cheating, a Violation of Academic Integrity Report will be submitted to the Dean of Graduate Studies to be placed in your university file, with copies provided to you, the department head, and the Dean of Rockefeller College. Additional penalties may include some combination of the following: revision and re-submission of the assignment, reduction of the grade or failure of the assignment, reduction of the course grade or failure of the course, filing of a case with the Office of Conflict Resolution and Civic Responsibility, suspension, or expulsion. For a more detailed description of the university‚Äôs academic honesty policies, visit the site.\n\n\n\n\n\n\nChatGPT and Other LLMs\n\n\n\nBy now, we are all aware of the technological advances in generative large language models (LLMs) trained on large quantities of written language scraped from around the internet. The University policy considers the use of ChatGPT and other generative LLMs to produce classwork without explicit permission from the instructor an act of plagiarism. I do not permit the use of ChatGPT or other generative LLMs in this course. First, generative LLMs can at times invent fictional sources, recombine information that is confidently stated but ultimately incorrect, and can produce generally mediocre and formulaic writing. Such events make the output unreliable - particularly for people aiming to be professionals working in the institutions that govern our society. Second, and more importantly, grappling with complicated trade-offs, collecting and synthesizing complex information thoughtfully, and going through the process of articulating your decisions and the knowledge base that inform them is a large part of an effective professional career. Learning in general is an arduous process that involves practice, trial and error, and confronting your current limits before finding ways to overcome them. In short, learning is work and the process by which that work occurs is often reading and writing, poorly at first and much better over time. I do think that, properly understood, LLMs can be a useful tool in managing routine tasks in which you have mastered the background, can detect and correct errors, and can use such tools effectively. However, early in your careers and in your academic lives, the very purpose of being in a graduate program is to have opportunities to learn new things (or old things in new ways) and using an LLM to do the work involved in learning will only cheat you of opportunities to learn, grow, and develop deeper and more lasting skills. Finally, and more practically, if you are caught using LLMs to produce the work assigned in this class, the work will be given a 0 and you will be cited for plagiarism.\n\n\n\n\n\nWe are committed to providing an accessible learning environment for all students. This includes students with physical, sensory, medical, cognitive, learning, mental health, and other disabilities. If you have, or think you may have a disability, please contact Disability Access and Inclusion Student Services (DAISS) by emailing daiss@albany.edu or calling 518 -442-5501. DAISS staff will explain the documentation and registration process, and set you up with an appointment. Once you have completed registration, you will be provided with a letter to inform your instructors that you are a student with a disability registered with DAISS, and which lists the recommended reasonable accommodations for your courses.\n\n\n\nThe Counseling Center (518-442-5800; 400 Patroon Creek Blvd, Suite 104) offers counseling and consultations regarding personal concerns, self-help information, and connections to off-campus resources. More information can be found at their site.\n\n\n\nSUNY-Albany offers a great collection available in several different media. Access to research help and library tutorials can be found online at the library‚Äôs site.\nFor information about SUNY-Albany‚Äôs Dewey Graduate Library, which is located on the Downtown Campus, visit their site.\n\n\n\nThe university offers a number of services for students who need assistance with writing and research projects. Support is available in the Writing Center (518-442-4061; 140 HU) and at the University Library. Information about the Writing Center can be found at their site.\n\n\n\nTitle IX of the Education Amendments of 1972 is a federal civil rights law that prohibits discrimination on the basis of sex in federally funded education programs and activities. The SUNY-wide Sexual Violence Prevention and Response Policies prohibit offenses defined as sexual harassment, sexual assault, intimate partner violence (dating or domestic violence), sexual exploitation, and stalking. The SUNY-wide Sexual Violence Prevention and Response Policies apply to the entire University at Albany community, including students, faculty, and staff of all gender identities. The University at Albany provides a variety of resources for support and advocacy to assist individuals who have experienced sexual offenses.\nConfidential support and guidance can be found through the Counseling Center (518-442-5800, or online), the University Health Center (518-442-5454, or online), and the Interfaith Center (518-489-8573, or online). Individuals at these locations will not report crimes to law enforcement or university officials without permission, except for in extreme circumstances, such as a health and/or safety emergency. Additionally, the Advocates at the University at Albany‚Äôs Advocacy Center for Sexual Violence are available to assist students without sharing information that could identify them (518-442-CARE, or online).\nSexual offenses can be reported non-confidentially to the Title IX Coordinator within The Office for Equity and Compliance (518-442-3800, or online, Building 25, Room 117) and/or the University Police Department (518-442-3131, or online).\n\n\n\n\n\n\nImportant\n\n\n\nPLEASE NOTE: Faculty members are considered ‚Äúresponsible employees‚Äù at the University at Albany, meaning that they are required to report all known relevant details about a complaint of sexual violence to the University‚Äôs Title IX Coordinator, including names of anyone involved or present, date, time, and location.\n\n\nIn case of an emergency, please call 911.\n\n\n\nA tentative grade given only when the student has nearly completed the course but due to circumstances beyond the student‚Äôs control the work is not completed on schedule. The date for the completion of the work is specified by the instructor. The date stipulated will not be later than one month before the end of the session following that in which the Incomplete is received. The grade I is automatically changed to E or U unless work is completed as agreed between the student and the instructor.\n\n\n\nStudents are excused, without penalty, to be absent because of religious beliefs, and will be provided equivalent opportunities for make-up examinations, study, or work requirements missed because of such absences. Students should notify the instructor of record in a timely manner, and the instructor will work directly with students to accommodate religious observances. Online courses will not schedule any assignment deadlines on religious holidays.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus504.html#footnotes",
    "href": "syllabus504.html#footnotes",
    "title": "Syllabus",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nPassword is RockCollege. Feel free to also use as a meeting spot with your groups - the rooms are private meeting rooms (this will make sense when you use it) and the space itself has a 25 person capacity.‚Ü©Ô∏é\nThis course is aligned with the five core competencies identified by the National Association of Schools of Public Affairs and Administration (NASPAA) as critical for success in public service careers. Competencies are integrative. Competencies are a bundle of knowledge, skills, abilities, and behaviors that, when fully integrated, define successful performance. Competencies are broader than knowing how to use Excel or being able to define what marginal cost means. Competencies describe the characteristics of the person who does the job best. In this way, competencies describe the whole person and their total performance. Competencies are broader than job tasks. The NASPAA identified competencies are 1) The ability to lead and manage in public governance; 2) To participate in and contribute to the policy process; 3) To analyze, synthesize, think critically, solve problems and make decisions; 4) To articulate and apply a public service perspective; 5) To communicate and interact productively with a diverse and changing workforce and citizenry.‚Ü©Ô∏é",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "assignments.html",
    "href": "assignments.html",
    "title": "Assignments",
    "section": "",
    "text": "Assignments for RPAD 504 will be posted here as they are assigned.",
    "crumbs": [
      "Assignments"
    ]
  },
  {
    "objectID": "labs.html",
    "href": "labs.html",
    "title": "Labs",
    "section": "",
    "text": "Labs can be found here as they are posted. We will go over them together in-class, but they will remain here as a reference and for review for problem sets and exams.",
    "crumbs": [
      "Labs"
    ]
  },
  {
    "objectID": "lab1.html",
    "href": "lab1.html",
    "title": "Lab 1 - Working With Stata",
    "section": "",
    "text": "Let‚Äôs start with some simple graphs in Stata. We will begin by looking at the distribution of time spent on homework across the country. The data used in this class exercise comes from the American Time Use Survey1, which I have used in my own research (you can read it here). But before we begin, let‚Äôs get a brief tour of Stata and set up our workspace for a productive flow of our work.",
    "crumbs": [
      "Labs",
      "Lab 1 - Working With Stata"
    ]
  },
  {
    "objectID": "lab1.html#windows",
    "href": "lab1.html#windows",
    "title": "Lab 1 - Working With Stata",
    "section": "Windows",
    "text": "Windows\nIf you are working with a Windows computer, you will start by clicking on the File Explorer () icon in the bottom of your screen and in the USB drive or primary drive you will be using, create a folder for the class and subfolders for data, do files, logs, and output.\n\nSelect the Drive for your class folder. \nSet up a class folder in the drive you chose. Name it rpad504. \nIn your class folder, set up a folder structure with these subfolders. Use the same names. \n\nNow, we are going to put the data for the class lab into our new data subfolder.\n\nOpen the data subfolder you just created and leave your file explorer open.\nIn your web browser, where you are reading this, right-click on the ‚ÄúLabs‚Äù tab at the top of the page and select ‚ÄúOpen in New Tab.‚Äù\nClick ‚ÄúData for Lab 1.‚Äù\nClick the Download () button, select ‚ÄúSave File‚Äù, and then click OK.\nIn Firefox, click the Blue Arrow in the top right and then click on the folder icon next to ‚Äúlab1.zip.‚Äù This will open your ‚ÄúDownloads‚Äù folder, which is the default place where Windows downloads files to your computer. Open ‚Äúlab1‚Äù and drag class1.dta to your still open data subfolder.\n\nNow we are all set! Leave your class folder open in the file explorer for now because we will need it in a moment. Each week, when you are doing assignments or labs, download the data using the same process and move it to your data folder. We will use paths to this folder in our .do files so that we can save our output and document what we‚Äôve done.",
    "crumbs": [
      "Labs",
      "Lab 1 - Working With Stata"
    ]
  },
  {
    "objectID": "lab1.html#macintosh",
    "href": "lab1.html#macintosh",
    "title": "Lab 1 - Working With Stata",
    "section": "Macintosh",
    "text": "Macintosh\nFor Mac users, I am in the process of hunting down a Mac to create the step-by-step screenshots to walk you through this process. However, the process is generally the same: create a class folder (rpad504) and the same four subfolders (do, data, output, and logs). Store data in the data folder after downloading it from the course site. The primary difference is finding paths for using in your .do files (see below). I found this guide helpful for finding paths on Macs; once you have it, you can copy and paste it into your first .do file, as outlined below, and then copy from there in future .do files.",
    "crumbs": [
      "Labs",
      "Lab 1 - Working With Stata"
    ]
  },
  {
    "objectID": "lab1.html#a-look-around-stata",
    "href": "lab1.html#a-look-around-stata",
    "title": "Lab 1 - Working With Stata",
    "section": "A Look Around Stata",
    "text": "A Look Around Stata\nNow let‚Äôs look around Stata for a moment and get a sense of the tool. To do this, open the data for Class Lab 1 that we just moved into our data folder. You should see a screen like this. You‚Äôll notice a few things. First, in the center of the screen is the output window of Stata\n\n\n\nOutput Window\n\n\nand this is where we will see the results of any and all analysis we do. It‚Äôs a lovely view and you will come to love it as much as I do! Untold truths will be found in this screen and, if you continue down this road, you might be the first to see a brand new, scientific result in this very view! Notice out in the center, there is already a use \"E:\\rpad316\\data\\class1.dta\" command that has been run. use tells Stata to open a data set and the path (in my case E:\\rpad316\\data\\) tells Stata where the data is stored and which data set (class1.dta) to open. Datasets in Stata are always .dta file types. Think of this as the same as .doc for word.\nOkay, off to the right, you can see the list of variables in this dataset.\n\n\n\nVariables\n\n\nHere, there are two columns. One lists the variable name of each variable in the dataset. This is the equivalent of a column header in a spreadsheet and is what we use to tell Stata to do things with that particular variable. The second is the Label of each variable. Labels are more detailed, plain language descriptions of the variable, often to remind us what the variable is and how it is measured. You can also see in the bottom right corner a description of the data. Note that the description tells us that there are 7,388 observations in this dataset.\nTo the left, you can see a running record of the commands that we run.\n\n\n\nHistory\n\n\nSuccessful commands will appear in black (or in my case white) text while unsuccessful commands will appear in red. ‚ÄúUnsuccessful commands‚Äù simply means Stata hit an error because of an error in your code.\n\n\n\n\n\n\nTip\n\n\n\nSome notes on Stata code. First, Stata is a literalist - it will only do what you tell it to and will only do exactly what you tell Stata to do. If you enter the wrong code or spell something wrong, Stata will just stop and do nothing else because it‚Äôs confused and has a sensitive soul. Second, Stata is case sensitive. When naming variables, use all lower case because in general, all commands and code in Stata will also be all lower case. You want to make your coding life as easy as possible and one way to avoid frustration is to name variables in a way that minimizes errors.\n\n\nFinally, we have the command window at the bottom.\n\n\n\nCommand\n\n\nThe command window is one of the ways you can tell Stata to do various things. The other way is through .do files, which we will get to in just a moment. Anything you enter into the command window will generate some sort of output in the output window and be recorded in the history window. If you make a mistake and get an error, you can click on the errant command in the history window and it will automatically be filled into the command window for you and then you can correct the error.",
    "crumbs": [
      "Labs",
      "Lab 1 - Working With Stata"
    ]
  },
  {
    "objectID": "lab1.html#footnotes",
    "href": "lab1.html#footnotes",
    "title": "Lab 1 - Working With Stata",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe American Time Use Survey (ATUS) is a nationally representative sample of Americans aged 15 and up. It is collected cross-sectionally (that is, with new respondents) every year. The ATUS collected time diary data from a person from each household that captures how respondents spent their time over the previous 24-hrs. It is tied to a subsample of the Current Population Survey (CPS), which allows researchers to examine a rich set of individual and household characteristics.‚Ü©Ô∏é\nWe will cover commenting in do files in a future class. Comments do not get run as commands in do files and are helpful for keeping yourself organized, remembering the purpose of different parts of your code, and helping collaborators understand what you were doing.‚Ü©Ô∏é",
    "crumbs": [
      "Labs",
      "Lab 1 - Working With Stata"
    ]
  },
  {
    "objectID": "lab1.html#working-in-stata",
    "href": "lab1.html#working-in-stata",
    "title": "Lab 1 - Working With Stata",
    "section": "Working in Stata",
    "text": "Working in Stata\nIn this class, and in general, it will be good for you to get used to working with .do files. In Stata, .do files are text files that include all of the code you are using in your project. A .do file can tell Stata what to do from start to finish and Stata will run every line of code in a .do file in order.2 To start a .do file, go to the top and click the ‚ÄúNew Do-File Editor‚Äù button () and a new .do file will open. First, let‚Äôs save this by going to ‚ÄúFile‚Äù -&gt; ‚ÄúSave As‚Ä¶‚Äù and save it in your class folder in the ‚Äúdo‚Äù subfolder. Name it ‚Äúclasslab1‚Äù so that you know what the do file does.\nNext, we are going to use the path to our file to set our command directory in Stata. For Windows users, getting the path is simple: go to the file explorer window where your class folder is open, click on the top bar in a space where there is no text, and you will see that the path is selected for you:\n\n\n\nPath Example\n\n\nFrom there, you can use CTRL + C to copy the file path.\nNow, return to your do file. In the first line of your do file, enter the code cd and then paste the path to your class folder (CTRL + V). The full line should read:\n\nStata\n\n\ncd \"C:\\rpad504\\\"\n\n\n\n\nYour path may be different, but the last part should be rpad504\\. The cd command tells Stata, ‚ÄúOkay, I am going to be working with files in this folder, so anytime I tell you to save or output something, use this as the initial path.‚Äù This will make our lives easier going foward and as long as you have a good file structure (the one I walked through here), you can simply put this line of code at the beginning of every do file and be good to go. Finally, before we begin, we will also set up a .log file. A log file tracks every command we enter and all of the output those commands produce. They are very handy tools that can keep track of what you‚Äôve done and how you did it. Researchers generally use .logs for record keeping purposes so that they can be transparent about their analysis later on down the road. Setting up a log file is easy. You simply tell Stata to start a .log and tell Stata where you would like that log to be stored.\n\nlocal today : di %tdCY.N.D date(\"$S_DATE\",\"DMY\")\nlog using \"logs/lab1_`today'.log\", replace\n\nFirst, the log command opens the log and the using command tells Stata where to create a log file to use as a log for what you‚Äôre about to do. In the quotations, the ‚Äúlogs‚Äù segment tells Stata to use the subfolder you set up in the class folder called ‚Äúlogs‚Äù (since we set up the command drive in the previous step, we only need to use the folder name!) and then the ‚Äúlab1.log‚Äù tells Stata to create a file in that folder named ‚Äúlab1‚Äù and to use it to log everything we do in our session. The , replace at the end of the command is important. It means anytime you run this .do file, it will update and overwrite the previous log file. This is a handy way to keep from having too many logs and losing track of them.\n\n\n\n\n\n\nTip\n\n\n\nWhen you are starting a new assignment or new lab, set up a new do file, save it in the .do folder, and copy and paste the cd code and the log code from your last do file. Then you just need to rename the log file in the code and you are ready to rumble.",
    "crumbs": [
      "Labs",
      "Lab 1 - Working With Stata"
    ]
  },
  {
    "objectID": "lab1.html#working-with-variables",
    "href": "lab1.html#working-with-variables",
    "title": "Lab 1 - Working With Stata",
    "section": "Working with Variables",
    "text": "Working with Variables\nLet‚Äôs get to work on some examples. Let‚Äôs drop the existing hhincome variable and recreate it from scratch. First, we can scroll down to the bottom of the variables list in the top right corner of our Stata window:\n\n\n\nVariable List Before Drop\n\n\nand see that hhincome is the last variable in the dataset. Let‚Äôs delete it using the drop command.\ndrop hhincome\nNow we can see that it is gone from our variables list: \nNow, to demonstrate how creating and coding variables works, we are going to create a categorical variable that groups all the different household income brackets we currently have as indicator variables. First, we will create a new variable and we will set it‚Äôs value as ‚Äúmissing.‚Äù In Stata, missing is signified using the . symbol.\ngen hhincome = .\nWe should see a message that 7388 missing values have been generated, and hhincome should reappear at the bottom of our variables list:\n\n\n\nVariable List After Generating A New Variable\n\n\nClicking on the browse icon () will pull up a spreadsheet-like display of our data. Scrolling all the way to the right of the spreadsheet should reveal our new variable column, full of empty space:\n\n\n\nBackend View of Dataset In Stata\n\n\nThen we are going to tell Stata to change the value of that variable to a different number for each category of household income. Note that the second part of the command has a double equal sign.\nreplace hhincome = 1 if loinc == 1\nreplace hhincome = 2 if inc2040 == 1\nreplace hhincome = 3 if inc4060 == 1\nreplace hhincome = 4 if inc6075 == 1\nreplace hhincome = 5 if inc75100 == 1\nreplace hhincome = 6 if inc100150 == 1\nreplace hhincome = 7 if inc150p == 1\nNow we have all of our categories coded into the same variable. When we return to browsing the data and scroll to the hhincome column, we should see:\n\n\n\nBackend View of Dataset In Stata\n\n\nTo keep ourselves sane, let‚Äôs go ahead and label the new variable and label the categories so we know what they mean later.\nlabel var hhincome \"Household income\"\nlabel define inccats 1 \"&lt;=20K\" 2 \"20-40K\" 3 \"40-60K\" 4 \"60-75K\" 5 \"75-100K\" 6 \"100-150K\" 7 \"&gt;=150K\"\nlabel val hhincome inccats\nFirst, after running the first line of code above, label var, the variable list should be showing hhincome now looks something like this:\n\n\n\nNew Variable Look\n\n\nSee how much clearer that label is for us? The label will also become the default in some tables and graphs, so it will ease the burden on us over time having our variables properly labeled.\nThe second two lines of code tell Stata to create a value label scheme that defines each category with a more informative label. In our case, this is telling Stata that category 1 includes people in households earning $20,000 or less and so on. The final line of code, label val, tells Stata to apply that value label scheme to our newly created variable, hhincome. The final results should look something like this:\n\n\n\nNew Variable Look in the Backend",
    "crumbs": [
      "Labs",
      "Lab 1 - Working With Stata"
    ]
  },
  {
    "objectID": "lab1.html#summarizing-data",
    "href": "lab1.html#summarizing-data",
    "title": "Lab 1 - Working With Stata",
    "section": "Summarizing Data",
    "text": "Summarizing Data\nFinally, we are going to look at the summary statistics, which includes the mean and the standard deviation, of homework time in minutes per day and hours per week for the full sample, for boys, and for girls.\nThe full sample:\nsum hw_tot weekly_hw\nWith an output that looks like this:\n. sum hw_tot weekly_hw\n\n    Variable |        Obs        Mean    Std. dev.       Min        Max\n-------------+---------------------------------------------------------\n      hw_tot |      7,388    46.95168    87.05461          0        875\n   weekly_hw |      2,842     14.2397    11.97486   .1166667   102.0833\nOn average, a high school student will spend 47 minutes on homework in a day, and, for those that do some homework, will spend about 14 hours per week on homework or studying. Let‚Äôs see how boys and girls compare in their homework time.\nHere‚Äôs only boys:\nsum hw_tot weekly_hw if male == 1\n\n    Variable |        Obs        Mean    Std. dev.       Min        Max\n-------------+---------------------------------------------------------\n      hw_tot |      3,828    39.00888    78.71242          0        690\n   weekly_hw |      1,310    13.29875    11.40719        .35       80.5\nAnd only girls:\nsum hw_tot weekly_hw if male == 0\n\n    Variable |        Obs        Mean    Std. dev.       Min        Max\n-------------+---------------------------------------------------------\n      hw_tot |      3,560    55.49242    94.48178          0        875\n   weekly_hw |      1,532    15.04429    12.38686   .1166667   102.0833\nWe can see that by telling Stata to only calculate the summary statistics for males, if male == 1, and for females, if male == 0, we can compare the average time spent on homework across student gender. Our sample shows that while boys spend about 39 minutes studying on an average day, girls spend 55 minutes studying on the average day. That‚Äôs quite a gap in study time!\nAs we have seen so far, Stata commands usually have options that can provide additional information or customization. We can use the detail option with the sum command to get the full set of statistics, including different percentiles and variance and measures of skew. The measure of skew tells us both the size of the disagreement between the mean and the median and the direction (a negative skew indicates the mean is lower than the median, while a positive skew indicates the mean is higher than the median).\nsum hw_tot weekly_hw, detail\n\n\n                           Daily T\n-------------------------------------------------------------\n      Percentiles      Smallest\n 1%            0              0\n 5%            0              0\n10%            0              0       Obs               7,388\n25%            0              0       Sum of wgt.       7,388\n\n50%            0                      Mean           46.95168\n                        Largest       Std. dev.      87.05461\n75%           60            690\n90%          150            760       Variance       7578.505\n95%          238            790       Skewness       2.775769\n99%          390            875       Kurtosis       13.31339\n\n               Weekly homework time (in hours)\n-------------------------------------------------------------\n      Percentiles      Smallest\n 1%     1.166667       .1166667\n 5%     2.333333            .35\n10%          3.5            .35       Obs               2,842\n25%            7            .35       Sum of wgt.       2,842\n\n50%         10.5                      Mean            14.2397\n                        Largest       Std. dev.      11.97486\n75%     18.08333           80.5\n90%     30.33333       88.66666       Variance       143.3973\n95%     38.26667       92.16666       Skewness       1.969594\n99%     58.91667       102.0833       Kurtosis       8.580481\nYou can also get a more targeted set of statistics using tabstat, which allows you to specify which statistics you want and which you do not. This will come in handy later when we start using Stata to create tables for us. Remember, the analyst‚Äôs goal is to find ways to focus on the analysis and not the producing of the analysis. When you find ways to automate making tables (and I will show you some over the semester), use them!\ntabstat hw_tot weekly_hw, statistics(mean sd)\n\n\n   Stats |    hw_tot  weekly~w\n---------+--------------------\n    Mean |  46.95168   14.2397\n      SD |  87.05461  11.97486\n------------------------------\nAnd always remember to include a line in your code that closes your log file.\nlog close\nYour first .do file should look like this:\ncd \"E:\\rpad504\\\"\nuse \"data\\class1.dta\"\nlocal today : di %tdCY.N.D date(\"$S_DATE\",\"DMY\")\nlog using \"logs/lab1_`today'.log\", replace\ndrop hhincome\ngen hhincome = .\nreplace hhincome = 1 if loinc == 1\nreplace hhincome = 2 if inc2040 == 1\nreplace hhincome = 3 if inc4060 == 1\nreplace hhincome = 4 if inc6075 == 1\nreplace hhincome = 5 if inc75100 == 1\nreplace hhincome = 6 if inc100150 == 1\nreplace hhincome = 7 if inc150p == 1\n\nlabel var hhincome \"Household income\"\nlabel define inccats 1 \"&lt;=20K\" 2 \"20-40K\" 3 \"40-60K\" 4 \"60-75K\" 5 \"75-100K\" 6 \"100-150K\" 7 \"&gt;=150K\"\nlabel val hhincome inccats\n\nsum hw_tot weekly_hw\n\nsum hw_tot weekly_hw if male == 1\nsum hw_tot weekly_hw if male == 0\n\nsum hw_tot weekly_hw, detail\n\ntabstat hw_tot weekly_hw, statistics(mean sd)\n\nsave \"data\\class1.dta\", replace\n\nlog close",
    "crumbs": [
      "Labs",
      "Lab 1 - Working With Stata"
    ]
  },
  {
    "objectID": "lab2.html",
    "href": "lab2.html",
    "title": "Lab 2 - Getting and Summarizing Data",
    "section": "",
    "text": "In this week‚Äôs lab, we will learn the basics of pulling data from a public data source (in this case, IPUMS), downloading the data in a format we want (in this case Stata), and using that data to create some variables and a summary table. We will be working with data from the American Community Survey and decennial census conducted by the U.S. Census Bureau. The data is rich and provides a great deal of information about the U.S. population in terms of income, demographics, and so on. For our lab, we will focus simply on the living conditions, demographics, family attributes, and income mix at various levels of total income. We‚Äôll start by using IPUMS to download the data, which requires an IPUMS login.",
    "crumbs": [
      "Labs",
      "Lab 2 - Getting and Summarizing Data"
    ]
  },
  {
    "objectID": "lab1.html#stata-1",
    "href": "lab1.html#stata-1",
    "title": "Lab 1 - Working With Stata",
    "section": "Stata",
    "text": "Stata\nlocal today : di %tdCY.N.D date(\"$S_DATE\",\"DMY\")\nlog using \"logs/lab1_`today'.log\", replace",
    "crumbs": [
      "Labs",
      "Lab 1 - Working With Stata"
    ]
  },
  {
    "objectID": "lab2.html#cleaning-the-data",
    "href": "lab2.html#cleaning-the-data",
    "title": "Lab 2 - Getting and Summarizing Data",
    "section": "Cleaning the Data",
    "text": "Cleaning the Data\nIn lab 1, we learned the basics of working with Stata and some rudimentary tools for data cleaning: creating a new categorical variable using a set of indicator variables. Here, we are going to learn some shortcuts to re-coding variables and creating sets of mutually exclusive indicators1 from a categorical variable. We are also going to create variables that are functions of other variables. Once we have the elements we need for a table, we are going to create a table that presents a variety of variables for the full sample, by the 90/10 income split, and by White and Non-White.\n\nUser-Written Packages\nFirst, we are going to install two packages that are not part of the standard Stata package set. Stata packages all get housed in an internet archive that can be accessed directly in Stata using the ssc install command. We will install fre, which is a user-written package that implements the tabulate command that we have already used in Stata but includes both variables codes and labels in the output. As you will come to see, this is very useful for getting a sense of how data is coded when we are at the cleaning stage of the data.\nWe are also going to install estout, which is actually a library of commands that all provide a very flexible way to make exportable tables from estimates stored in Stata‚Äôs temporary memory.\nTo do this, run:\nssc install fre, replace\nssc install estout, replace\n\n\nCreating New Variables\nFor this exercise, we are going to look at the characteristics we have overall and separately by income and some rough binaries of race and ethnicity. To do that, we will want to recode some variables using a combination of the recode command, attaching an option to the tab command, and creating a variable that is a function of other variables. Let‚Äôs start with the tab command because it‚Äôs quite straightforward.\nWhen we are making tables with categorical variables in them, it‚Äôs quite useful to have indicator variables for each category. We could do this using the gen and replace route that we took in lab 1, but an alternate route is to use the tab command and attach the , gen(stub) option. Let‚Äôs do this for race using the variable rachsing to get five indicator variables for each category of race.\ntab rachsing, gen(racecat)\nNote that the racecat part in the gen option is telling Stata to name the indicator variables for each category racecat# where # is the number of the category.\nYour output should look like this:\n. tab rachsing, gen(racecat)\n\n             race: simplified |\nrace/ethnicity identification |      Freq.     Percent        Cum.\n------------------------------+-----------------------------------\n                        white |  6,634,498       68.35       68.35\n       black/african american |    974,520       10.04       78.39\namerican indian/alaska native |     84,561        0.87       79.26\n       asian/pacific islander |    591,554        6.09       85.35\n              hispanic/latino |  1,421,558       14.65      100.00\n------------------------------+-----------------------------------\n                        Total |  9,706,691      100.00\nand we should have a new series of indicators in our variable list:\n\n\n\nBottom of Variables List Panel\n\n\nWe will go ahead and do the same thing for marital status (marst) and biological sex (sex).\ntab marst, gen(marital)\ntab sex, gen(bsex)\nNow let‚Äôs move on to sex, educational attainment, and transit mode taken to work. Let‚Äôs use fre to take a look at these categorical variables.\nfre educ tranwork\nHere, we get a series of tables that look like:\n. fre educ tranwork\n\neduc -- educational attainment [general version]\n----------------------------------------------------------------------------------\n                                     |      Freq.    Percent      Valid       Cum.\n-------------------------------------+--------------------------------------------\nValid   0  n/a or no schooling       |     778620       5.95       5.95       5.95\n        1  nursery school to grade 4 |     934263       7.14       7.14      13.10\n        2  grade 5, 6, 7, or 8       |     828813       6.34       6.34      19.43\n        3  grade 9                   |     266110       2.03       2.03      21.47\n        4  grade 10                  |     292384       2.24       2.24      23.70\n        5  grade 11                  |     328913       2.51       2.51      26.22\n        6  grade 12                  |    3873003      29.61      29.61      55.83\n        7  1 year of college         |    1488821      11.38      11.38      67.21\n        8  2 years of college        |     882180       6.74       6.74      73.95\n        10 4 years of college        |    2075039      15.86      15.86      89.82\n        11 5+ years of college       |    1331923      10.18      10.18     100.00\n        Total                        |   1.31e+07     100.00     100.00           \n----------------------------------------------------------------------------------\n\ntranwork -- means of transportation to work\n----------------------------------------------------------------------------------------------------\n                                                       |      Freq.    Percent      Valid       Cum.\n-------------------------------------------------------+--------------------------------------------\nValid   0  n/a                                         |    7103702      54.31      54.31      54.31\n        10 auto, truck, or van                         |    4813653      36.80      36.80      91.11\n        20 motorcycle                                  |       8748       0.07       0.07      91.18\n        31 bus                                         |      64683       0.49       0.49      91.67\n        32 bus or trolley bus                          |      28865       0.22       0.22      91.89\n        34 light rail, streetcar, or trolley (carro    |       4002       0.03       0.03      91.92\n           p√É¬∫blico in pr)                             |                                            \n        35 streetcar or trolley car (publico in puerto |        816       0.01       0.01      91.93\n           rico, 2000)                                 |                                            \n        36 subway or elevated                          |      79579       0.61       0.61      92.54\n        37 long-distance train or commuter train       |      26096       0.20       0.20      92.74\n        38 taxicab                                     |      10578       0.08       0.08      92.82\n        39 ferryboat                                   |       2567       0.02       0.02      92.84\n        50 bicycle                                     |      28359       0.22       0.22      93.05\n        60 walked only                                 |     162531       1.24       1.24      94.30\n        70 other                                       |      59606       0.46       0.46      94.75\n        80 worked at home                              |     686284       5.25       5.25     100.00\n        Total                                          |   1.31e+07     100.00     100.00           \n----------------------------------------------------------------------------------------------------\nNotice how many categories there are? That‚Äôs probably going to be overwhelming to look at in a table all together. We might want to collapse these categories into higher level categories. To do this, we‚Äôll use a new command: recode. This will be much simpler than our previous route of creating a new variable and then replacing each value of the variable with a new value for each category. How tedious!\nWe‚Äôll start with educ since it‚Äôs the most straightforward. Here, we might really only be concerned with a breakdown of people with no college, some college, and a Bachelor‚Äôs degree or more. Maybe we go an extra mile and include a separate breakout for graduate degrees. Here, we‚Äôll use recode to define a smaller set of categories that include multiple categories from the original variable.\nrecode educ (0/5 = 1 \"Less than HS\") (6 = 2 \"HS\") (7/8 = 3 \"Some college\") (10 = 4 \"College degree\") (11 = 5 \"More than Bachelor's\"), gen(seduc)\nNow let‚Äôs do the same with mode of transportation taken to work. We can set up a simple set of categories for walking, biking, taking a car, and taking mass transit (broadly defined). Then we will create a separate category for worked at home and other.\nrecode tranwork (10/20 38 = 1 \"Motor vehicle\") (31/37 39 = 2 \"Mass transit\") (50 = 3 \"Bicycle\") (60 = 4 \"Walked\") (70 = 5 \"Other\") (80 = 6 \"WFH\"), gen(stransit)\nNow let‚Äôs go ahead and use our tab command to create indicators from our now shortened categorical variables.\ntab seduc, gen(seduc)\ntab stransit, gen(transit)\nFinally, we are going to work with continuous variables to create some variables that are a function of other variables. In particular, we want to see a little bit about how people with different incomes vary in the source of their income (e.g., wages, cash transfer programs, public benefits, etc.). To do that, we are going to take advantage of the rich set of income variables provided by the ACS. Let‚Äôs start by dealing with some quirks of the income data. The total personal income variable can be negative and includes a code of 9999998 for missing values and 9999999 for cases where income is not applicable for collection. Let‚Äôs go ahead and change those to missing.\nreplace inctot = .a if inctot &gt;= 9999998\n\n\n\n\n\n\nCodebook\n\n\n\nAlways check the codebook tabs for variables you will be using! I did the grunt work for the lab here and recorded the appropriate missing codes, but these kinds of subtle interpretations of variables can really bias your answers if you don‚Äôt check the codebook ahead of time and understand how varaibles are coded in your data.\n\n\nNow, we‚Äôre going to have to do that for all the income source variables too.2 Given that there‚Äôs several of them, this could be tedious to code. But this makes for a good opportunity to learn a little trick: a foreach loop. See, this loop will let you apply the same process to many variables at once, perfect for situations where the same data change needs to be applies to many variables. To do it, we will put all of the variables in a foreach command and label them something simple, like ‚ÄúX‚Äù, and then use that stand-in in our code. It will look like this:\nforeach x of varlist incwage incbus00 incss incwelfr incinvst incretir incsupp incother{\n    replace `x' = 0 if `x' &gt;= 999998\n}\nNow we have our cleaned variables and we can calculate the percent of total income that comes from the different sources captured by the ACS. Here again, we are applying the same formula over and over, so we can use a foreach loop here too.\nforeach x of varlist incwage incbus00 incss incwelfr incinvst incretir incsupp incother{\n    gen `x'_prc = ((`x' / inctot)*100)\n}\nAn alternative way to code the above would be to simply generate each variable on it‚Äôs own line with a gen command. That would look like this:\ngen incwage_prc = ((incwage/inctot)*100)\ngen incbus00_prc = ((incbus00/inctot)*100)\ngen incss_prc = ((incss/inctot)*100)\ngen incwelfr_prc = ((incwelfr/inctot)*100)\ngen incinvst_prc = ((incinvst/inctot)*100)\ngen incretir_prc = ((incretir/inctot)*100)\ngen incsupp_prc = ((incsupp/inctot)*100)\ngen incother_prc = ((incother/inctot)*100)\nBoth routes will get you the same set of variables describing the proportion of personal inctome from various sources among Americans.\nFinally, we are interested in looking at how these variables look at different points of the income distribution, so we should have some categorical variables that capture different points in the income distribution! Here, we are going to look at different points in the distribution of family income and Stata makes this really easy for us. We are going to use the xtile command which tells State to rank all observations on a given variable, identify the number of quantiles you need, and create a new categorical variable that identifies which quantile each observation is in. We‚Äôre going to look at the 90th and 10th percentile, so we will want to specify 10 quantiles. We will also look at White and Non-White splits, so we can code our Non-White indicator here too.\nreplace ftotinc = .a if ftotinc &gt;= 9999998\nxtile finc_q = ftotinc, nq(10)\ngen nonwhite = 0\nreplace nonwhite = 1 if rachsing &gt; 1",
    "crumbs": [
      "Labs",
      "Lab 2 - Getting and Summarizing Data"
    ]
  },
  {
    "objectID": "lab2.html#footnotes",
    "href": "lab2.html#footnotes",
    "title": "Lab 2 - Getting and Summarizing Data",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n‚ÄúMutually exclusive‚Äù means that each observation can only be in one category. Ideally, the sum of the categories adds up to 100 percent.‚Ü©Ô∏é\nThe income source variables have a slightly different coding schema - 6 digits instead of 7 - but the same general formula.‚Ü©Ô∏é\nThis avoids hitting errors when you already created a table of the same name, but be careful! If you are creating multiple tables in a single .do file, this can overwrite all previous tables with just that last one you created. Just be sure your outputs have unique names in your code.‚Ü©Ô∏é",
    "crumbs": [
      "Labs",
      "Lab 2 - Getting and Summarizing Data"
    ]
  },
  {
    "objectID": "lab2.html#making-tables",
    "href": "lab2.html#making-tables",
    "title": "Lab 2 - Getting and Summarizing Data",
    "section": "Making Tables",
    "text": "Making Tables\nWe are now ready to make our first table! To do so, we are going to draw on some things we‚Äôve already learned. First, we are going to use the sum command. Second, we are going to use three commands from the estout package we installed earlier: estpost to tell Stata to post the estimates from the next command into its temporary memory, est sto to tell Stata to label those stored estimates with a name we give it, and esttab to tell Stata to put some of those estimates into a nice table for us. When we do this, we will run the sum command for the full sample and then for the sub-samples we want.\nFirst, let‚Äôs return to what we are trying to do here. Our goal from the beginning was to look at how people in the top and bottom of the income distribution differ in terms of education, family characteristics, sources of income, and - just out of curiosity - mode of transportation used for commuting to work (among those employed). Here, we are functionally just trying to describe differences between people in the top decile of income and those in the bottom decile of income. We refer to this as descriptive analysis and we often describe the world with summary statistics.\nLet‚Äôs start with a simple table of just average income and the percent of income from wages (incwage_prc, which we created with our second loop in the previous section). To do this we will start with estimating summary statistics of those two variables using sum, but we will add our new command estpost so that Stata stores these estimates in its memory.\n\n\n\n\n\n\nNote\n\n\n\nThe sum command in Stata estimates a whole set of statistics for all of the variables in the variable list you enter with the command. These statistics are all different ways of summarizing the central tendency and spread of the variable in your sample. These include the five-number summary of the distribution (the minimum value, 25th percentile, median, 75th percentile, and maximum value), the mean, the standard deviation, and the variance of the variable (and several others - see the output from adding , detail like we did in lab 1). All of these statistics can be stored in Stata‚Äôs memory to be recalled later for things like making tables or making figures of data.\n\n\nestpost sum inctot incwage_prc\nWe will get output from something that looks like this:\n. estpost sum inctot incwage_prc\n\n             |  e(count)   e(sum_w)    e(mean)     e(Var)      e(sd)     e(min)     e(max)     e(sum) \n-------------+----------------------------------------------------------------------------------------\n      inctot |    168171     168171   46004.99   4.81e+09   69371.54      -8500    1309000   7.74e+09 \n incwage_prc |    147621     147621   63.64085   503210.3   709.3732      -1000     200000    9394726 \nOur sample average income is about $46,005 per year and about 63% of that income comes from wages and salary for the average person in the sample. Let‚Äôs go ahead and store this under a label so we can put it in a table.\nest sto all\nIn the code about, est sto is my command and it‚Äôs short for ‚Äúestimates store.‚Äù The all part of the command is a label for the estimates for future reference. You can use any label you like (no spaces), but I typicall keep them short and snappy so that referencing them later is easy. In this call, we are estimating summary statistics of income and percent of income from wages using the full sample - or all observations - so I use the label all for clarity.\nNow, let‚Äôs make our first table with just these two variables and just their averages.\nesttab all using \"output\\table1a.csv\", cell(mean) replace\nHere, esttab is the command for creating an ‚Äúestimates table‚Äù containing the estimates stored under the label all. The using part of the code tells Stata to create a .csv file named table1a in the output subfolder of our command drive (in this case, the rpad504 class folder). CSV files can be opened and edited in Excel and copied and pasted into a word doc. The options I have after the comma tell Stata to enter the means of the variables stored under all inthe cells of the table I am creating and to replace any file in output named table1a.csv with the new table I am making with this line of code.3 Remember that all is the label I have for all of the summary statistics of two variables for all observations in the sample. We will expand our variables list and samples in a table shortly.\n\n\n\n\n\n\nNote\n\n\n\nI am keeping things simple by using a format that works cleanly in Excel. You can also used the ‚Äú.rtf‚Äù extension for a ‚Äúrich text file‚Äù that can be opened directly in Word and also copied and pasted into a report or document you are using. There are more advanced file formats too that can really expand how much you can directly output your table into a document and print it into a formal report that is already properly formatted, but that is a little more advanced. I am just flagging this here because you would still use this base code for creating these tables in Stata.\n\n\nBut first, our simple table should look like this: \nNotice a few things. First, this is very basic. Second, the variables are just listed with their variable names - not very attractive or intuitive. We can have Stata use variable labels to clean that up a bit for us for variables we have labled. Finally, this doesn‚Äôt let us compare high and low income households. So in the next version of the table, we are going to add the rest of the income variables, add the label option, and add columns for high- and low-income sub-samples.\nestpost sum inctot incwage_prc incbus00_prc incss_prc incwelfr_prc incinvst_prc incretir_prc incsupp_prc incother_prc\nest sto all\nestpost sum inctot incwage_prc incbus00_prc incss_prc incwelfr_prc incinvst_prc incretir_prc incsupp_prc incother_prc if finc_q == 1\nest sto low\nestpost sum inctot incwage_prc incbus00_prc incss_prc incwelfr_prc incinvst_prc incretir_prc incsupp_prc incother_prc if finc_q == 10\nest sto high\nesttab all low high using \"output\\table1b.csv\", cell(mean) label replace\nNotice a few things in the code. First, I estimate the same summary statistics on the same set of variables overall and for my two subsamples. Two get summary statistics for my subsamples I simply add if conditions. This will give me ‚Äúconditional means‚Äù or the mean of income and percent of income from various sources conditional on being from a household in the bottom (or top) decile of family income. Second, notice that while I add lines of code to estimate statistics for each subsample, I still only have one line for creating the table. Now, I am simply updating the line to add the labels for including low and high - my labels for the summary statistics for low-income households and high-income households, respectively. Finally, I added the label option to tell Stata to use variable labels in the table rather than the less intuitive and details variable names. My table now looks like this:\n\n\n\nTable 1b\n\n\nNow we have 3 columns, helpfully labeled by Stata, showing the means of all of our variables overall and separately by decile. And we can see that the label option used the label for inctot instead of the variable name. Unfortunately, we never assigned variable labels for the labels we created! I present this so you can see the downstream usefulness of always adding labels to variables when you create them. I‚Äôd also point out that the label for inctot is sort of weak - it does not describe units and is all lower case. We‚Äôd typically like to see something more polished in a real table. Let‚Äôs re-label our variables now in Stata.\nlabel var inctot \"Total personal income ($)\"\nlabel var incwage_prc \"% of income from wages/salary\"\nlabel var incbus00_prc \"% of income from business or farm revenue\"\nlabel var incss_prc \"% of income from Social Security benefits\"\nlabel var incwelfr_prc \"% of income from cash welfare benefits\"\nlabel var incinvst_prc \"% of income from investment returns\"\nlabel var incretir_prc \"% of income from retirement income\"\nlabel var incsupp_prc \"% of income from supplemental security income\"\nlabel var incother_prc \"% of income from other sources\"\nNow, after running the code to label our variables more clearly, let‚Äôs re-run the same table command.\nesttab all low high using \"output\\table1b.csv\", cell(mean) label replace\nOur table looks much better!\n\n\n\nTable 1b V2\n\n\nOne final set of edits and I think we‚Äôll have a nice table. For readers, it‚Äôs not at all obvious how and why columns 1, 2 and 3 are different from one another. We know, but they likely don‚Äôt. And we also might want to know a little something about the spread of our variables. And we definitely don‚Äôt need so many decimals after the decimal place in our table presentation of estiamtes. In the final stage of our table making, we are going to add some options to label our columns, add standard deviations to our cells, and format the stats in our cells. Once again, we are just going to edit our last line of code, because everything else has already been done.\nesttab all low high using \"output\\table1c.csv\", cell(mean(fmt(2)) sd(fmt(2) par)) unstack compress mtitles(\"All\" \"Bottom 10%\" \"Top 10%\") label replace \nIn the code above, we use sd to add standard deviations to our table. If we were just including a new statistic in the cells of our table, we could just add it in the parentheses to read cell(mean sd). But we wanted to also modify the formats here, so next to each statistic, we add parentheses for options applied to that statistic (e.g., mean() and sd()). In those parentheses, we add fmt(2) to say ‚ÄúStata, I want this statistics to have a format that rounds to the second decimal point. Since we have two statistics now, we want to differentiate them, so we follow convention and put our measure of spread - the standard deviation - in parentheses as well by also including the par piece of our code. Our final piece of estimate presentation formatting, we add options unstack and compress to tell Stata that the table can stack the standard deviations underneath the means for each variable and make sure there are no blank spaces between variables. This keeps our table compact and clean. After dealing with our presentation of our estimates, we include the mtitles (short for model titles) command to add labels to each column in our table. Since we have a column for each sample - the full sample, the low-income sample, and the high-income sample - we can make our labels‚ÄùAll,‚Äù ‚ÄúBottom 10%,‚Äù and ‚ÄúTop 10%.‚Äù\n\n\n\n\n\n\nTip\n\n\n\nIn practice, most tables follow very similar formats and you are often only changing a few pieces of this base code for a table at a time. Obviously the variables in the table will change across contexts, but things like formatting and what statistics to include in the cells will often be pretty constant across contexts.\n\n\n\n\n\nTable 1c\n\n\nLook at that! A nice, formatted table worthy of sharing with minimal edits needed to format it. Our rows provide clear, easily interpretable descriptions of the variables, our columns have numeric headers for reference and labels to provide readers a quick understanding of what sample is included in each column, and we present both means and standard deviations for our variables.\n\n\n\n\n\n\nTip\n\n\n\nWhen writing, it‚Äôs useful to have your columns numbered in your tables so that you can discuss results and draw reader attention to a column with the results you‚Äôd like to discuss. For instance, if this was table 1 in a report, I might write, ‚ÄúAs a comparison of columns 2 and 3 in Table 1 shows, people in the bottom decile of household income have a much higher share of their earnings from cash transfer programs, such as Social Security and welfare benefits, relative to people in the top decile.‚Äù\n\n\nOur full list of variables can also be added to the code and we can add two more columns to our table - White and Non-White - for some additional descriptive analysis of sub-samples of policy interest.\nIt will look something like this:\nestpost sum inctot incwage_prc incbus00_prc incss_prc incwelfr_prc incinvst_prc incretir_prc incsupp_prc incother_prc racecat1 racecat2 racecat3 racecat4 racecat5 seduc1 seduc2 seduc3 seduc4 seduc5 transit1 transit2 transit3 transit4 transit5 transit6 transit7 marital1 marital2 marital3 marital4 marital5 marital6 bsex1 bsex2\nest sto all\nestpost sum inctot incwage_prc incbus00_prc incss_prc incwelfr_prc incinvst_prc incretir_prc incsupp_prc incother_prc racecat1 racecat2 racecat3 racecat4 racecat5 seduc1 seduc2 seduc3 seduc4 seduc5 transit1 transit2 transit3 transit4 transit5 transit6 transit7 marital1 marital2 marital3 marital4 marital5 marital6 bsex1 bsex2 if finc_q == 1\nest sto low\nestpost sum inctot incwage_prc incbus00_prc incss_prc incwelfr_prc incinvst_prc incretir_prc incsupp_prc incother_prc racecat1 racecat2 racecat3 racecat4 racecat5 seduc1 seduc2 seduc3 seduc4 seduc5 transit1 transit2 transit3 transit4 transit5 transit6 transit7 marital1 marital2 marital3 marital4 marital5 marital6 bsex1 bsex2 if finc_q == 10\nest sto high\nestpost sum inctot incwage_prc incbus00_prc incss_prc incwelfr_prc incinvst_prc incretir_prc incsupp_prc incother_prc racecat1 racecat2 racecat3 racecat4 racecat5 seduc1 seduc2 seduc3 seduc4 seduc5 transit1 transit2 transit3 transit4 transit5 transit6 transit7 marital1 marital2 marital3 marital4 marital5 marital6 bsex1 bsex2 if racecat1 == 1\nest sto white\nestpost sum inctot incwage_prc incbus00_prc incss_prc incwelfr_prc incinvst_prc incretir_prc incsupp_prc incother_prc racecat1 racecat2 racecat3 racecat4 racecat5 seduc1 seduc2 seduc3 seduc4 seduc5 transit1 transit2 transit3 transit4 transit5 transit6 transit7 marital1 marital2 marital3 marital4 marital5 marital6 bsex1 bsex2 if nonwhite == 1\nest sto nonwhite\nesttab all low high white nonwhite using \"output\\table2.csv\", cell(mean(fmt(2)) sd(fmt(2) par)) unstack compress mtitles(\"All\" \"Bottom 10%\" \"Top 10%\" \"White\" \"Non-White\") label replace",
    "crumbs": [
      "Labs",
      "Lab 2 - Getting and Summarizing Data"
    ]
  },
  {
    "objectID": "probset1.html",
    "href": "probset1.html",
    "title": "Problem Set 1",
    "section": "",
    "text": "This problem set will require 3 file submissions: 1) a clean .do file with the commands used to answer the questions, 2) a .log file with the all the output from the commands used to answer the questions, and 3) a clean word document with the answers to the questions. You will submit all three files via Brightspace.\nFor this problem set, you will need to use the Current Population Survey data from IPUMS. Use the ‚ÄúGet Data‚Äù feature and in the ASEC tab select only the 2023 sample.\n\n\n\n\n\n\nImportant\n\n\n\nNote that you will need to also open the ‚ÄúBasic Monthly‚Äù tab and deselect all of those samples as well.\n\n\nFor this assignment, you will be using four variables to describe health, employment, income, health expenditures. In parentheses below are the menus paths to find the listed variables. Complete your dataset by adding:\n\nHEALTH (Person -&gt; Annual Social and Economic Supplement -&gt; Disability)\nFTOTVAL (Person -&gt; Annual Social and Economic Supplement -&gt; Income)\nEMPSTAT (Person -&gt; Core -&gt; Work)\nMOOP (Person -&gt; Annual Social and Economic Supplement -&gt; Health Insurance)\n\nThis should leave you with 4 variables in 1 sample. Use the data to answer the following questions.",
    "crumbs": [
      "Assignments",
      "Problem Set 1"
    ]
  },
  {
    "objectID": "probset1.html#instructions",
    "href": "probset1.html#instructions",
    "title": "Problem Set 1",
    "section": "",
    "text": "This problem set will require 3 file submissions: 1) a clean .do file with the commands used to answer the questions, 2) a .log file with the all the output from the commands used to answer the questions, and 3) a clean word document with the answers to the questions. You will submit all three files via Brightspace.\nFor this problem set, you will need to use the Current Population Survey data from IPUMS. Use the ‚ÄúGet Data‚Äù feature and in the ASEC tab select only the 2023 sample.\n\n\n\n\n\n\nImportant\n\n\n\nNote that you will need to also open the ‚ÄúBasic Monthly‚Äù tab and deselect all of those samples as well.\n\n\nFor this assignment, you will be using four variables to describe health, employment, income, health expenditures. In parentheses below are the menus paths to find the listed variables. Complete your dataset by adding:\n\nHEALTH (Person -&gt; Annual Social and Economic Supplement -&gt; Disability)\nFTOTVAL (Person -&gt; Annual Social and Economic Supplement -&gt; Income)\nEMPSTAT (Person -&gt; Core -&gt; Work)\nMOOP (Person -&gt; Annual Social and Economic Supplement -&gt; Health Insurance)\n\nThis should leave you with 4 variables in 1 sample. Use the data to answer the following questions.",
    "crumbs": [
      "Assignments",
      "Problem Set 1"
    ]
  },
  {
    "objectID": "probset1.html#questions",
    "href": "probset1.html#questions",
    "title": "Problem Set 1",
    "section": "Questions",
    "text": "Questions\n\nWhat is the average and median family income in the sample?\n\n\n\n\n\n\n\nTip\n\n\n\nBe sure you account for the code used for missing values in any variables you are using to answer these questions when relevant.\n\n\n\nWhat is the average family income among families in the bottom and top quartiles?\nWhat is the distribution of self-reported health status overall, in the bottom quartile of family income, and in the top quartile of family income? How would you describe the difference between people from low and high income families in self-reported health status?\nWhat is the difference in average out of pocket spending on health costs between people with excellent health and people with poor health?\nWhat is the difference in average out of pocket spending on health costs between people in the top decile of family income and people in the bottom decile of family income?\nCreate a table that shows the distribution of health status, average out of pocket spending on health, and average family income in the sample overall and separately by the top and bottom quartiles of family income. Paste the table from the csv output into word.\n\n\n\n\n\n\n\nTip\n\n\n\nYour table should have 7 variable rows and 3 columns of statistics.\n\n\n\nExtra Credit (worth 1 point)\n\nCreate a new table starting from the table you made for question 6, but add the percent of family income spent on out of pocket costs on health care as a new variable in the table.",
    "crumbs": [
      "Assignments",
      "Problem Set 1"
    ]
  },
  {
    "objectID": "lab3.html",
    "href": "lab3.html",
    "title": "Lab 3 - Simple Visuals",
    "section": "",
    "text": "For this lab, we are going to go over how to summarize data visually using a few simple graphs. We will be using data from the National Health Interview Survey - a nationally representative, cross-sectional survey collected every year by the National Institutes of Health. I have created a dataset for this lab using data from 2018, 2019, 2022, and 2023 to allow us to explore recent health trends and some differences before and after the pandemic. Here, we are going to answer a handful of questions:\n\nWhat does the distribution of sexual orientation look like in the U.S. and has it changed over time?\nDo indicators of physical and mental health look different before and after the pandemic?\nHas there been a change in related behaviors, like flu vaccine take-up rates?\n\nIn the dataset found here, I have captured some general demographic information, employment status information, educational attainment, and then some health and health insurance indicators.\nLet‚Äôs start our .do file with our usual starting sequence. Download the data, move it to your data subfolder (but don‚Äôt open it!), open a new do file in Stata and type:\ncd \"C:\\rpad504\\\"\nuse \"data\\nhis_00004.dta\"\nlocal today : di %tdCY.N.D date(\"$S_DATE\",\"DMY\")\nlog using \"logs/lab3_`today'.log\", replace\nMake sure the path in the first line points to your class folder, highlight all four lines, and hit execute. Now we‚Äôre ready to go!\n\n\n\n\nWe will start with a few different ways to look at the distribution of sexual orientation. Why? Well, recall that this is survey data that is self-reported. Shares of the population that are willing to self-report their sexual orientation may change over time alongside broader social norms around tolerance and acceptance of different orientations. We will look at the data to see if that pattern seems to show up even in this small window of data (a five year period between 2018 and 2023). But we are also looking at this first because it provides a simple example of describing a categorical variable. Recall that in making tables, we are generally not interested in describing the mean of a categorical variable because it‚Äôs not particularly meaningful. Instead, we want to see how observations are distributed across categories of the variable (or, put more directly, the distribution of the variable). The same is true for visualizing data.\nBefore we begin analyzing the data, we want to make sure we understand what the variable looks like and whether it needs to be cleaned at all. Let‚Äôs start by running our trusty fre command and look at the categories of the variable.\nfre sexorien\nAfter which, we should see‚Ä¶\nsexorien -- sexual orientation\n---------------------------------------------------------------------------------------------\n                                                |      Freq.    Percent      Valid       Cum.\n------------------------------------------------+--------------------------------------------\nValid   0 niu                                   |      71763      38.51      38.51      38.51\n        1 lesbian or gay                        |       2075       1.11       1.11      39.62\n        2 straight, that is, not lesbian or gay |     104866      56.27      56.27      95.90\n        3 bisexual                              |       1863       1.00       1.00      96.90\n        4 something else                        |        600       0.32       0.32      97.22\n        5 i don't know the answer               |        925       0.50       0.50      97.72\n        7 unknown-refused                       |        780       0.42       0.42      98.13\n        8 unknown-not ascertained               |       3478       1.87       1.87     100.00\n        Total                                   |     186350     100.00     100.00           \n---------------------------------------------------------------------------------------------\nNote that there‚Äôs some values that we don‚Äôt really want to include in our analysis. Categories like ‚ÄúUnknown‚Äù and ‚ÄúNIU‚Äù are generally going to be treated as ‚Äúmissing‚Äù in our analysis because either the respondents weren‚Äôt asked that question (niu, or ‚Äúnot in universe‚Äù) or refused to respond for whatever reason (both ‚Äúunknown‚Äù categories). For our purposes, we really don‚Äôt know their orientation and so will simply exclude them from our description for now by treating them as missing. Let‚Äôs use our recode command to create a new variable that sets those categories to missing and relables the existing categories more cleanly.\nrecode sexorien (0 7/8 = .a) (1 = 1 \"Lesbian or gay\") (2 = 2 \"Straight\") (3 = 3 \"Bisexual\") (4 = 4 \"Something else\") (5 = 5 \"I don't know\"), gen(orientation)\nlabel var orientation \"Sexual orientation\"\nNotice that I also went ahead and relabeled the new variable so that I have a nice, clean label for future figures and tables.\n\n\n\nWe‚Äôll start with my least favorite way of displaying data and perhaps the way you encounter data quite often in reports: the pie graph. The pie graph provides a circle that represents all observations in a sample and divides the circle into slices that reflect the proportion of observations in different categories of a categorical variable. Let‚Äôs look at sexual orientation.\ngraph pie, over(orientation)\ngraph export \"output\\orientation_pie.png\", as(png) replace\n\n\n\nPie chart of sexual orientation in the US\n\n\nLet‚Äôs add a title so our readers know what they‚Äôre looking at in this chart.\ngraph pie, over(orientation)  title(\"Sexual orientation in USA, 2018-2019 and 2022-2023\")\ngraph export \"output\\orientation_pie.png\", as(png) replace\n\n\n\nPie chart of sexual orientation in the US, NOW WITH TITLE!\n\n\n\n\n\nAlright, as I said, I don‚Äôt really like pie charts. There are very few instances where I find a pie chart more clear to read than a bar chart. Worse, pie charts are less flexible for comparisons over time or across two different variables. So, let‚Äôs start with a simple bar chart of the same variable.\ngraph bar, over(orientation) title(\"Sexual orientation in USA, 2018-2019 and 2022-2023\")\ngraph export \"output\\orientation_bar.png\", as(png) replace\n\n\n\nBar graph of sexual orientation\n\n\nNotice that the code is very similar. Here, we are telling Stata to create a bar graph and to plot the proportion of responses over the categories in the variable orientation, which we created earlier. The flexibility here is we can also add other variables to the graph, such as the earnings or BMI or any other continuous variable, to look at the average of that variable over each of the categories in orientation as well. But for now, we are interested in looking at the distribution in sexual orientation over time. Now, looking at bar graphs for four years might be a little overwhelming, so let‚Äôs look at a bar graph for just two years: 2018 and 2023. Let‚Äôs start by creating a simple variable that only includes 2018 and 2023.\ngen twoyear = 2018 if year == 2018\nreplace twoyear = 2023 if year == 2023\nNow we can use that new variable and the by option to tell Stata to make a bar graph over the categories of orientation and separately by the values in twoyear. I‚Äôm also going to add an option - asyvars - to use different colors for each category of orientation so that comparison across years is a little easier on the eyes.\ngraph bar, over(orientation) by(twoyear) asyvars\ngraph export \"output\\orientation_bar2.png\", as(png) replace\n\n\n\nBar graph of sexual orientation in 2018 and 2023\n\n\nIf we squint, we can sort of see an uptick the non-straight categories, suggesting at least some marginal additional willingness to self-report sexual orientation.\n\n\n\nConventional bar graphs, like the ones we just made, often have the bars running vertically. There are a few drawbacks to this orientation, however. First, pages are often longer than they are wide - both in their digital presentation online and in their printed form on paper. Practically, this means we have more space constraints in how much information we can present at once in the bar graph. Second, and related, the labels on the x-axis begin to face severe trade-offs between clarity in the label and the space available for a legible label. Longer, more precise labels of variables in our graph will begin to overlap with other variable labels, leaving our graph an illegible mess. We can do things like shortening our labels or angling our labels to a 45 degree angle, but the former has limits and the latter distorts our graphs space and makes reading the labels less natural.\nA quick example:\ngraph bar, over(orientation, label(angle(45))) title(\"Sexual orientation in USA, 2018-2019 and 2022-2023\")\ngraph export \"output\\orientation_bar3.png\", as(png) replace\nNotice, in the code, I added an option to the presentation of orientation in the graph line of the code (, label(angle(45)))). This tells Stata to tilt the labels of the variable 45 degrees. It can be a quick fix to long labels, but‚Ä¶\n\n\n\nBar graph of sexual orientation with angled label\n\n\nnotice that the graph area shrank a bit. You can imagine that even longer labels will distort the graph further. And, again, it‚Äôs not really natural to read at a 45 degree angle and our purpose in data visualization is to make things easier on readers, not more challenging.\nFortunately, there‚Äôs a pretty easy alternative: horizontal bar graphs. Horizontal bar graphs allow our graph to expand down the length of the page rather than rellying on the width, giving us more real estate to work with. They also re-orient our graph to the y-axis, allowing us to use slightly more specific labels if we wanted while still keeping them in a natural reading orientation. In Stata, the starting point for a horizontal bar graph is as simple as adding an h to our code:\ngraph hbar, over(orientation) title(\"Sexual orientation in USA, 2018-2019 and 2022-2023\")\ngraph export \"output\\orientation_bar4.png\", as(png) replace\nNotice that in the code above, I simply replaced bar with hbar (and I also dropped the now unnecessary angle option). This gives us a graph that looks like this:\n\n\n\nHorizontal bar graph of sexual orientation\n\n\nIn many cases, the horizontal bar graph is the better option, but that is partly subject to context and partly subject to taste, so it‚Äôs good to know how to work with both presentations of data.\n\n\n\n\nNow we can move on to question 2, which deals with indicators of physical and mental health before and after the pandemic. For this, we are going to use days of work lost to health issues, self-reported health status, and the frequency of feeling depressed. Let‚Äôs start by looking at the categorical variables of health status and frequency of feeling depressed.\nfre health\nfre depfreq\nWhich will look something like this:\nhealth -- health status\n--------------------------------------------------------------------------\n                             |      Freq.    Percent      Valid       Cum.\n-----------------------------+--------------------------------------------\nValid   1 excellent          |      59916      32.15      32.15      32.15\n        2 very good          |      58186      31.22      31.22      63.38\n        3 good               |      45864      24.61      24.61      87.99\n        4 fair               |      17053       9.15       9.15      97.14\n        5 poor               |       5194       2.79       2.79      99.93\n        7 unknown-refused    |         89       0.05       0.05      99.97\n        9 unknown-don't know |         48       0.03       0.03     100.00\n        Total                |     186350     100.00     100.00           \n--------------------------------------------------------------------------\n\nhealth -- health status\n--------------------------------------------------------------------------\n                             |      Freq.    Percent      Valid       Cum.\n-----------------------------+--------------------------------------------\nValid   1 excellent          |      59916      32.15      32.15      32.15\n        2 very good          |      58186      31.22      31.22      63.38\n        3 good               |      45864      24.61      24.61      87.99\n        4 fair               |      17053       9.15       9.15      97.14\n        5 poor               |       5194       2.79       2.79      99.93\n        7 unknown-refused    |         89       0.05       0.05      99.97\n        9 unknown-don't know |         48       0.03       0.03     100.00\n        Total                |     186350     100.00     100.00           \n--------------------------------------------------------------------------\n\ndepfreq -- how often feel depressed\n-------------------------------------------------------------------------------\n                                  |      Freq.    Percent      Valid       Cum.\n----------------------------------+--------------------------------------------\nValid   0 niu                     |      53813      28.88      28.88      28.88\n        1 daily                   |       5283       2.83       2.83      31.71\n        2 weekly                  |       7454       4.00       4.00      35.71\n        3 monthly                 |       9118       4.89       4.89      40.61\n        4 a few times a year      |      34401      18.46      18.46      59.07\n        5 never                   |      73370      39.37      39.37      98.44\n        7 unknown-refused         |        280       0.15       0.15      98.59\n        8 unknown-not ascertained |       2231       1.20       1.20      99.79\n        9 unknown-don't know      |        400       0.21       0.21     100.00\n        Total                     |     186350     100.00     100.00           \n-------------------------------------------------------------------------------\nAgain we can see that both variables need some cleaning to collapse categories into things that are missing and not missing. Let‚Äôs go ahead and do that now.\nrecode health (7/9 = .a) (1 = 1 \"Excellent\") (2 = 2 \"Very good\") (3 = 3 \"Good\") (4 = 4 \"Fair\") (5 = 5 \"Poor\"), gen(health_nm)\nrecode depfreq (0 7/9 = .a) (1 = 1 \"Daily\") (2 = 2 \"Weekly\") (3 = 3 \"Monthly\") (4 = 4 \"A few times/yr\") (5 = 5 \"Never\"), gen(depfreq_nm)\nrecode empstat (0 220 997/999 = .a) (100/120 = 1 \"Employed\") (200 = 0 \"Unemployed\"), gen(employed)\nrecode wldayr (996/999 = .a), gen(wlday_nm)\ngen workingage = 0\nreplace workingage = 1 if age &lt;= 55 & age &gt;= 25\nlabel var wlday_nm \"Work loss days past year\"\nlabel var employed \"Employed\"\nlabel var health_nm \"Health status\"\nlabel var depfreq_nm \"Frequency feeling depressed\"\nI also included the relevant recode of the continuous variable of work days lost to health issues in the last 12 months and a recode of employment status so we can focus in on prime age workers. Let‚Äôs start with the same ideas we‚Äôve already worked with - the distribution of categorical variables in a bar graph. We can start with both health overall and frequency of feeling depressed overall.\ngraph bar, over(health_nm)\ngraph export \"output\\health_bar1.png\", as(png) replace\ngraph bar, over(depfreq_nm)\ngraph export \"output\\depression_bar1.png\", as(png) replace\nAnd those graphs should look a little something like this:\n\n\n\nDistribution of health status in the US\n\n\nand this‚Ä¶\n\n\n\nDistribution of frequency of feeling depressed in the US\n\n\nNow, let‚Äôs look at them before and after COVID. First, we need to create a variable that identifies the pre-COVID and post-COVID periods in our data.\ngen post_covid = 0\nreplace post_covid = 1 if year &gt;= 2020\nlabel define covid 0 \"Pre-COVID\" 1 \"Post-COVID\"\nlabel val post_covid covid\nThen we can use the same structure we used in our by year approach for sexual orientation.\ngraph bar, over(health_nm) by(post_covid)\ngraph export \"output\\health_bar2.png\", as(png) replace\ngraph bar, over(depfreq_nm) by(post_covid)\ngraph export \"output\\depression_bar2.png\", as(png) replace\n\n\n\nHealth status before and after COVID\n\n\nLooking at health status, we can see some marginal changes, mostly for the worse, in the post-pandemic period.\n\n\n\nDepression frequency before and after COVID\n\n\nAnd same with depression. In general, the post bars shift leftward toward a higher proportion of Americans feeling depressed more often in the post-COVID period than the pre-period.\nBut what about a more concrete and continuous measure like days of work missed for health reasons? This is a count variable, which means we can look at changes in averages over time. This means, we could do two things. First, we could look at the average days of work missed by health status in the pre- and post-COVID periods. Then, we could also look at the year-to-year average number of days of work missed in our sample.\nFirst, we‚Äôll start with the bar graph because it‚Äôs sort of the simplest.\ngraph bar (mean) wlday_nm, over(health_nm) by(post_covid) asyvars\ngraph export \"output\\health_bar3.png\", as(png) replace\nNotice that we added a variable before the options and specified that we wanted the mean of that variable. This tells Stata to give use the conditional mean of the variable (wlday_nm) for each category in the health status variable (health_nm), separately by COVID period.\n\n\n\nAverage number of missed workdays by health status\n\n\nNow, let‚Äôs take advantage of the working age and employment variables we got to create a nice line graph of the aver days of work missed each year in our data. To do this, we will need to create a new variable that is equal to the sample mean for each year. But we‚Äôll also restrict this to only include workers in the prime age working years and who report being employed. While this sounds complicated, this is quite simple in Stata and takes advantage of the egen command - a command that allows you to generate variables that are functions of other variables (in this case, a mean).\negen dayslost_mn = mean(wlday_nm) if workingage == 1 & employed == 1, by(year)\nlabel var dayslost_mn \"Avg. Days of Work Lost\"\nIn this code, I tell Stata to create a new variabel, dayslost_mn that is the mean of wlday_nm for each year in the data (the by(year) part of the code). Now we can simply plot our averages for each year in our data with a straightforward command:\ntwoway connected dayslost_mn year\ngraph export \"output\\dayslost_line1.png\", as(png) replace\n\n\n\nAverage number of days of work missed for health reasons\n\n\nNote that the dots reflect years of actual, observed data, meaning there‚Äôs a gap of missing data in our dataset. A jump of nearly an entire additional lost day of work for health reasons on average is a pretty big jump!\n\n\n\n\n\nFinally, we are going to look at whether a related behavioral change - taking the flu vaccine - was altered by the COVID pandemic. Perhaps more people take flu precautions from the heightened awareness brought on by exposure to a novel widespread respiratory illness.\nTo do this, let‚Äôs look at the flu vaccine variable to see how it‚Äôs coded.\nfre vacflu12m\nvacflu12m -- had any flu vaccine, past 12 months\n-----------------------------------------------------------------------\n                          |      Freq.    Percent      Valid       Cum.\n--------------------------+--------------------------------------------\nValid   0 niu             |      39145      21.01      21.01      21.01\n        1 no              |      71805      38.53      38.53      59.54\n        2 yes             |      72703      39.01      39.01      98.55\n        7 refused         |        209       0.11       0.11      98.66\n        8 not ascertained |       1559       0.84       0.84      99.50\n        9 don't know      |        929       0.50       0.50     100.00\n        Total             |     186350     100.00     100.00           \n-----------------------------------------------------------------------\nNotice that, again, we are going to need to recode. Let‚Äôs go ahead and recode it into a binary and then recode the other categories into missings.\nrecode vacflu12m (0 7/9 = .a) (1 = 0 \"No\") (2 = 1 \"Yes\"), gen(fluvax)\nlabel var fluvax \"Got flu vaccine in past year\"\nNow, just like with days of work lost per week, we want averages of our binary variable by year, which will give us the annual proportion of the population that took the flu vaccine at some point during the year.\negen fluvax_mn = mean(fluvax), by(year)\nlabel var fluvax_mn \"Prc. Received Flu Vax.\"\nAnd now the code is the same as before.\ntwoway connected fluvax_mn year\ngraph export \"output\\fluvax_line1.png\", as(png) replace\n\n\n\nPercent of Americans receiving the Flu vaccine over time\n\n\n\n\n\nOften, we aren‚Äôt just interested in one group of people or one category of data at a time. Sometimes we want to monitor trends in multiple groups or one multiple measures at once. Let‚Äôs use the categorical measures of health as an example. For this, we have a little additional data cleaning setup to do, but the process is about the same - calculating the average of each variable of interest by year and then plotting the averages. Let‚Äôs get started with the data cleaning:\ntab health_nm, gen(healthcat)\negen hlth_excel_mn = mean(healthcat1), by(year)\negen hlth_vg_mn = mean(healthcat2), by(year)\negen hlth_g_mn = mean(healthcat3), by(year)\negen hlth_f_mn = mean(healthcat4), by(year)\negen hlth_p_mn = mean(healthcat5), by(year)\nAbove, we create an indicator variable for each category of the variable health named with a stub healthcat. This gives us five indicator variables. We then use those indicators to calculate the proportion of people in each category for each year in our sample.\n\n\n\n\n\n\nIndicator Variables\n\n\n\nRemember that indicator variables function as a true/false switch that takes a value of 1 if the observation falls into the category the variable is flagging and 0 if the observation does not. For instance, if I had 3 categories of education - high school or less, some college, and college or more - there would be three indicator variables (hs, somecol, col). For people with a high school diploma or less, the variable hs would be 1 and would be 0 for all others. For people who took some college classes but never got a 4-year degree, the variable somecol would be 1 and 0 for all others. And finally, people with a 4-year degree or more, the variable col would be 1 and 0 for all others. If we know everyone‚Äôs educational attainment, everyone will have exactly one of these variables coded as 1 and the others set to 0 in their row of our dataset. Since the average of a variable is \\(\\overline{X} = \\frac{\\sum x_i}{n}\\), the average of an indicator is the number of 1 values divided by the sample size. Or, put differently, the proportion of the sample in the indicator variable‚Äôs category!\n\n\nNow, let‚Äôs label these variables so that they make sense in our graph:\nlabel var hlth_excel_mn \"Prc. Excellent Health\"\nlabel var hlth_vg_mn \"Prc. Very Good Health\"\nlabel var hlth_g_mn \"Prc. Good Health\"\nlabel var hlth_f_mn \"Prc. Fair Health\"\nlabel var hlth_p_mn \"Prc. Poor Health\"\nAlright, now we are ready to graph this. The code for the graph is simple: we use the same command as before but we list all of our y-axis variables and then close with our x-axis variable.\ntwoway connected hlth_excel_mn hlth_vg_mn hlth_g_mn hlth_f_mn hlth_p_mn year\ngraph export \"output\\health_line1.png\", as(png) replace\nAnd we get a graph that looks like this:\n\n\n\nLine graph of trends of self-reported health status over time\n\n\nNow, because we based this on the means of indicators, our y-axis is on a 0 to 1 scale. Percents are, of course, on a 0-100 scale, and readers will find it mentally taxing to re-scale this in their minds themselves. So we might want to add an extra data cleaning step before we make our graph that scales the means appropriately because, as always, our job is to make reader‚Äôs lives easier.\ngen hlth_excel_mn100 = hlth_excel_mn*100\ngen hlth_vg_mn100 = hlth_vg_mn*100\ngen hlth_g_mn100 = hlth_g_mn*100\ngen hlth_f_mn100 = hlth_f_mn*100\ngen hlth_p_mn100 = hlth_p_mn*100\n\nlabel var hlth_excel_mn100 \"Prc. Excellent Health\"\nlabel var hlth_vg_mn100 \"Prc. Very Good Health\"\nlabel var hlth_g_mn100 \"Prc. Good Health\"\nlabel var hlth_f_mn100 \"Prc. Fair Health\"\nlabel var hlth_p_mn100 \"Prc. Poor Health\"\nNotice that the new variables simply multiply our original means by 100 (the * symbol in Stata is multiplication) to re-scale the variables to a 100 point scale. This will make our graph‚Äôs labels and range more intuitive to understand for most readers.\ntwoway connected hlth_excel_mn100 hlth_vg_mn100 hlth_g_mn100 hlth_f_mn100 hlth_p_mn100 year\ngraph export \"output\\health_line2.png\", as(png) replace\n\n\n\nRe-scaled line graph of trends of self-reported health status over time\n\n\nAnd that‚Äôs all for this lab! Let‚Äôs close our .do file with our usual log close and exit Stata.",
    "crumbs": [
      "Labs",
      "Lab 3 - Simple Visuals"
    ]
  },
  {
    "objectID": "lab3.html#proportions",
    "href": "lab3.html#proportions",
    "title": "Lab 3 - Simple Visuals",
    "section": "",
    "text": "We will start with a few different ways to look at the distribution of sexual orientation. Why? Well, recall that this is survey data that is self-reported. Shares of the population that are willing to self-report their sexual orientation may change over time alongside broader social norms around tolerance and acceptance of different orientations. We will look at the data to see if that pattern seems to show up even in this small window of data (a five year period between 2018 and 2023). But we are also looking at this first because it provides a simple example of describing a categorical variable. Recall that in making tables, we are generally not interested in describing the mean of a categorical variable because it‚Äôs not particularly meaningful. Instead, we want to see how observations are distributed across categories of the variable (or, put more directly, the distribution of the variable). The same is true for visualizing data.\nBefore we begin analyzing the data, we want to make sure we understand what the variable looks like and whether it needs to be cleaned at all. Let‚Äôs start by running our trusty fre command and look at the categories of the variable.\nfre sexorien\nAfter which, we should see‚Ä¶\nsexorien -- sexual orientation\n---------------------------------------------------------------------------------------------\n                                                |      Freq.    Percent      Valid       Cum.\n------------------------------------------------+--------------------------------------------\nValid   0 niu                                   |      71763      38.51      38.51      38.51\n        1 lesbian or gay                        |       2075       1.11       1.11      39.62\n        2 straight, that is, not lesbian or gay |     104866      56.27      56.27      95.90\n        3 bisexual                              |       1863       1.00       1.00      96.90\n        4 something else                        |        600       0.32       0.32      97.22\n        5 i don't know the answer               |        925       0.50       0.50      97.72\n        7 unknown-refused                       |        780       0.42       0.42      98.13\n        8 unknown-not ascertained               |       3478       1.87       1.87     100.00\n        Total                                   |     186350     100.00     100.00           \n---------------------------------------------------------------------------------------------\nNote that there‚Äôs some values that we don‚Äôt really want to include in our analysis. Categories like ‚ÄúUnknown‚Äù and ‚ÄúNIU‚Äù are generally going to be treated as ‚Äúmissing‚Äù in our analysis because either the respondents weren‚Äôt asked that question (niu, or ‚Äúnot in universe‚Äù) or refused to respond for whatever reason (both ‚Äúunknown‚Äù categories). For our purposes, we really don‚Äôt know their orientation and so will simply exclude them from our description for now by treating them as missing. Let‚Äôs use our recode command to create a new variable that sets those categories to missing and relables the existing categories more cleanly.\nrecode sexorien (0 7/8 = .a) (1 = 1 \"Lesbian or gay\") (2 = 2 \"Straight\") (3 = 3 \"Bisexual\") (4 = 4 \"Something else\") (5 = 5 \"I don't know\"), gen(orientation)\nlabel var orientation \"Sexual orientation\"\nNotice that I also went ahead and relabeled the new variable so that I have a nice, clean label for future figures and tables.\n\n\n\nWe‚Äôll start with my least favorite way of displaying data and perhaps the way you encounter data quite often in reports: the pie graph. The pie graph provides a circle that represents all observations in a sample and divides the circle into slices that reflect the proportion of observations in different categories of a categorical variable. Let‚Äôs look at sexual orientation.\ngraph pie, over(orientation)\ngraph export \"output\\orientation_pie.png\", as(png) replace\n\n\n\nPie chart of sexual orientation in the US\n\n\nLet‚Äôs add a title so our readers know what they‚Äôre looking at in this chart.\ngraph pie, over(orientation)  title(\"Sexual orientation in USA, 2018-2019 and 2022-2023\")\ngraph export \"output\\orientation_pie.png\", as(png) replace\n\n\n\nPie chart of sexual orientation in the US, NOW WITH TITLE!\n\n\n\n\n\nAlright, as I said, I don‚Äôt really like pie charts. There are very few instances where I find a pie chart more clear to read than a bar chart. Worse, pie charts are less flexible for comparisons over time or across two different variables. So, let‚Äôs start with a simple bar chart of the same variable.\ngraph bar, over(orientation) title(\"Sexual orientation in USA, 2018-2019 and 2022-2023\")\ngraph export \"output\\orientation_bar.png\", as(png) replace\n\n\n\nBar graph of sexual orientation\n\n\nNotice that the code is very similar. Here, we are telling Stata to create a bar graph and to plot the proportion of responses over the categories in the variable orientation, which we created earlier. The flexibility here is we can also add other variables to the graph, such as the earnings or BMI or any other continuous variable, to look at the average of that variable over each of the categories in orientation as well. But for now, we are interested in looking at the distribution in sexual orientation over time. Now, looking at bar graphs for four years might be a little overwhelming, so let‚Äôs look at a bar graph for just two years: 2018 and 2023. Let‚Äôs start by creating a simple variable that only includes 2018 and 2023.\ngen twoyear = 2018 if year == 2018\nreplace twoyear = 2023 if year == 2023\nNow we can use that new variable and the by option to tell Stata to make a bar graph over the categories of orientation and separately by the values in twoyear. I‚Äôm also going to add an option - asyvars - to use different colors for each category of orientation so that comparison across years is a little easier on the eyes.\ngraph bar, over(orientation) by(twoyear) asyvars\ngraph export \"output\\orientation_bar2.png\", as(png) replace\n\n\n\nBar graph of sexual orientation in 2018 and 2023\n\n\nIf we squint, we can sort of see an uptick the non-straight categories, suggesting at least some marginal additional willingness to self-report sexual orientation.\n\n\n\nConventional bar graphs, like the ones we just made, often have the bars running vertically. There are a few drawbacks to this orientation, however. First, pages are often longer than they are wide - both in their digital presentation online and in their printed form on paper. Practically, this means we have more space constraints in how much information we can present at once in the bar graph. Second, and related, the labels on the x-axis begin to face severe trade-offs between clarity in the label and the space available for a legible label. Longer, more precise labels of variables in our graph will begin to overlap with other variable labels, leaving our graph an illegible mess. We can do things like shortening our labels or angling our labels to a 45 degree angle, but the former has limits and the latter distorts our graphs space and makes reading the labels less natural.\nA quick example:\ngraph bar, over(orientation, label(angle(45))) title(\"Sexual orientation in USA, 2018-2019 and 2022-2023\")\ngraph export \"output\\orientation_bar3.png\", as(png) replace\nNotice, in the code, I added an option to the presentation of orientation in the graph line of the code (, label(angle(45)))). This tells Stata to tilt the labels of the variable 45 degrees. It can be a quick fix to long labels, but‚Ä¶\n\n\n\nBar graph of sexual orientation with angled label\n\n\nnotice that the graph area shrank a bit. You can imagine that even longer labels will distort the graph further. And, again, it‚Äôs not really natural to read at a 45 degree angle and our purpose in data visualization is to make things easier on readers, not more challenging.\nFortunately, there‚Äôs a pretty easy alternative: horizontal bar graphs. Horizontal bar graphs allow our graph to expand down the length of the page rather than rellying on the width, giving us more real estate to work with. They also re-orient our graph to the y-axis, allowing us to use slightly more specific labels if we wanted while still keeping them in a natural reading orientation. In Stata, the starting point for a horizontal bar graph is as simple as adding an h to our code:\ngraph hbar, over(orientation) title(\"Sexual orientation in USA, 2018-2019 and 2022-2023\")\ngraph export \"output\\orientation_bar4.png\", as(png) replace\nNotice that in the code above, I simply replaced bar with hbar (and I also dropped the now unnecessary angle option). This gives us a graph that looks like this:\n\n\n\nHorizontal bar graph of sexual orientation\n\n\nIn many cases, the horizontal bar graph is the better option, but that is partly subject to context and partly subject to taste, so it‚Äôs good to know how to work with both presentations of data.",
    "crumbs": [
      "Labs",
      "Lab 3 - Simple Visuals"
    ]
  },
  {
    "objectID": "lab3.html#pre-post-comparisons",
    "href": "lab3.html#pre-post-comparisons",
    "title": "Lab 3 - Simple Visuals",
    "section": "",
    "text": "Now we can move on to question 2, which deals with indicators of physical and mental health before and after the pandemic. For this, we are going to use days of work lost to health issues, self-reported health status, and the frequency of feeling depressed. Let‚Äôs start by looking at the categorical variables of health status and frequency of feeling depressed.\nfre health\nfre depfreq\nWhich will look something like this:\nhealth -- health status\n--------------------------------------------------------------------------\n                             |      Freq.    Percent      Valid       Cum.\n-----------------------------+--------------------------------------------\nValid   1 excellent          |      59916      32.15      32.15      32.15\n        2 very good          |      58186      31.22      31.22      63.38\n        3 good               |      45864      24.61      24.61      87.99\n        4 fair               |      17053       9.15       9.15      97.14\n        5 poor               |       5194       2.79       2.79      99.93\n        7 unknown-refused    |         89       0.05       0.05      99.97\n        9 unknown-don't know |         48       0.03       0.03     100.00\n        Total                |     186350     100.00     100.00           \n--------------------------------------------------------------------------\n\nhealth -- health status\n--------------------------------------------------------------------------\n                             |      Freq.    Percent      Valid       Cum.\n-----------------------------+--------------------------------------------\nValid   1 excellent          |      59916      32.15      32.15      32.15\n        2 very good          |      58186      31.22      31.22      63.38\n        3 good               |      45864      24.61      24.61      87.99\n        4 fair               |      17053       9.15       9.15      97.14\n        5 poor               |       5194       2.79       2.79      99.93\n        7 unknown-refused    |         89       0.05       0.05      99.97\n        9 unknown-don't know |         48       0.03       0.03     100.00\n        Total                |     186350     100.00     100.00           \n--------------------------------------------------------------------------\n\ndepfreq -- how often feel depressed\n-------------------------------------------------------------------------------\n                                  |      Freq.    Percent      Valid       Cum.\n----------------------------------+--------------------------------------------\nValid   0 niu                     |      53813      28.88      28.88      28.88\n        1 daily                   |       5283       2.83       2.83      31.71\n        2 weekly                  |       7454       4.00       4.00      35.71\n        3 monthly                 |       9118       4.89       4.89      40.61\n        4 a few times a year      |      34401      18.46      18.46      59.07\n        5 never                   |      73370      39.37      39.37      98.44\n        7 unknown-refused         |        280       0.15       0.15      98.59\n        8 unknown-not ascertained |       2231       1.20       1.20      99.79\n        9 unknown-don't know      |        400       0.21       0.21     100.00\n        Total                     |     186350     100.00     100.00           \n-------------------------------------------------------------------------------\nAgain we can see that both variables need some cleaning to collapse categories into things that are missing and not missing. Let‚Äôs go ahead and do that now.\nrecode health (7/9 = .a) (1 = 1 \"Excellent\") (2 = 2 \"Very good\") (3 = 3 \"Good\") (4 = 4 \"Fair\") (5 = 5 \"Poor\"), gen(health_nm)\nrecode depfreq (0 7/9 = .a) (1 = 1 \"Daily\") (2 = 2 \"Weekly\") (3 = 3 \"Monthly\") (4 = 4 \"A few times/yr\") (5 = 5 \"Never\"), gen(depfreq_nm)\nrecode empstat (0 220 997/999 = .a) (100/120 = 1 \"Employed\") (200 = 0 \"Unemployed\"), gen(employed)\nrecode wldayr (996/999 = .a), gen(wlday_nm)\ngen workingage = 0\nreplace workingage = 1 if age &lt;= 55 & age &gt;= 25\nlabel var wlday_nm \"Work loss days past year\"\nlabel var employed \"Employed\"\nlabel var health_nm \"Health status\"\nlabel var depfreq_nm \"Frequency feeling depressed\"\nI also included the relevant recode of the continuous variable of work days lost to health issues in the last 12 months and a recode of employment status so we can focus in on prime age workers. Let‚Äôs start with the same ideas we‚Äôve already worked with - the distribution of categorical variables in a bar graph. We can start with both health overall and frequency of feeling depressed overall.\ngraph bar, over(health_nm)\ngraph export \"output\\health_bar1.png\", as(png) replace\ngraph bar, over(depfreq_nm)\ngraph export \"output\\depression_bar1.png\", as(png) replace\nAnd those graphs should look a little something like this:\n\n\n\nDistribution of health status in the US\n\n\nand this‚Ä¶\n\n\n\nDistribution of frequency of feeling depressed in the US\n\n\nNow, let‚Äôs look at them before and after COVID. First, we need to create a variable that identifies the pre-COVID and post-COVID periods in our data.\ngen post_covid = 0\nreplace post_covid = 1 if year &gt;= 2020\nlabel define covid 0 \"Pre-COVID\" 1 \"Post-COVID\"\nlabel val post_covid covid\nThen we can use the same structure we used in our by year approach for sexual orientation.\ngraph bar, over(health_nm) by(post_covid)\ngraph export \"output\\health_bar2.png\", as(png) replace\ngraph bar, over(depfreq_nm) by(post_covid)\ngraph export \"output\\depression_bar2.png\", as(png) replace\n\n\n\nHealth status before and after COVID\n\n\nLooking at health status, we can see some marginal changes, mostly for the worse, in the post-pandemic period.\n\n\n\nDepression frequency before and after COVID\n\n\nAnd same with depression. In general, the post bars shift leftward toward a higher proportion of Americans feeling depressed more often in the post-COVID period than the pre-period.\nBut what about a more concrete and continuous measure like days of work missed for health reasons? This is a count variable, which means we can look at changes in averages over time. This means, we could do two things. First, we could look at the average days of work missed by health status in the pre- and post-COVID periods. Then, we could also look at the year-to-year average number of days of work missed in our sample.\nFirst, we‚Äôll start with the bar graph because it‚Äôs sort of the simplest.\ngraph bar (mean) wlday_nm, over(health_nm) by(post_covid) asyvars\ngraph export \"output\\health_bar3.png\", as(png) replace\nNotice that we added a variable before the options and specified that we wanted the mean of that variable. This tells Stata to give use the conditional mean of the variable (wlday_nm) for each category in the health status variable (health_nm), separately by COVID period.\n\n\n\nAverage number of missed workdays by health status\n\n\nNow, let‚Äôs take advantage of the working age and employment variables we got to create a nice line graph of the aver days of work missed each year in our data. To do this, we will need to create a new variable that is equal to the sample mean for each year. But we‚Äôll also restrict this to only include workers in the prime age working years and who report being employed. While this sounds complicated, this is quite simple in Stata and takes advantage of the egen command - a command that allows you to generate variables that are functions of other variables (in this case, a mean).\negen dayslost_mn = mean(wlday_nm) if workingage == 1 & employed == 1, by(year)\nlabel var dayslost_mn \"Avg. Days of Work Lost\"\nIn this code, I tell Stata to create a new variabel, dayslost_mn that is the mean of wlday_nm for each year in the data (the by(year) part of the code). Now we can simply plot our averages for each year in our data with a straightforward command:\ntwoway connected dayslost_mn year\ngraph export \"output\\dayslost_line1.png\", as(png) replace\n\n\n\nAverage number of days of work missed for health reasons\n\n\nNote that the dots reflect years of actual, observed data, meaning there‚Äôs a gap of missing data in our dataset. A jump of nearly an entire additional lost day of work for health reasons on average is a pretty big jump!",
    "crumbs": [
      "Labs",
      "Lab 3 - Simple Visuals"
    ]
  },
  {
    "objectID": "lab3.html#proportions-over-time",
    "href": "lab3.html#proportions-over-time",
    "title": "Lab 3 - Simple Visuals",
    "section": "",
    "text": "Finally, we are going to look at whether a related behavioral change - taking the flu vaccine - was altered by the COVID pandemic. Perhaps more people take flu precautions from the heightened awareness brought on by exposure to a novel widespread respiratory illness.\nTo do this, let‚Äôs look at the flu vaccine variable to see how it‚Äôs coded.\nfre vacflu12m\nvacflu12m -- had any flu vaccine, past 12 months\n-----------------------------------------------------------------------\n                          |      Freq.    Percent      Valid       Cum.\n--------------------------+--------------------------------------------\nValid   0 niu             |      39145      21.01      21.01      21.01\n        1 no              |      71805      38.53      38.53      59.54\n        2 yes             |      72703      39.01      39.01      98.55\n        7 refused         |        209       0.11       0.11      98.66\n        8 not ascertained |       1559       0.84       0.84      99.50\n        9 don't know      |        929       0.50       0.50     100.00\n        Total             |     186350     100.00     100.00           \n-----------------------------------------------------------------------\nNotice that, again, we are going to need to recode. Let‚Äôs go ahead and recode it into a binary and then recode the other categories into missings.\nrecode vacflu12m (0 7/9 = .a) (1 = 0 \"No\") (2 = 1 \"Yes\"), gen(fluvax)\nlabel var fluvax \"Got flu vaccine in past year\"\nNow, just like with days of work lost per week, we want averages of our binary variable by year, which will give us the annual proportion of the population that took the flu vaccine at some point during the year.\negen fluvax_mn = mean(fluvax), by(year)\nlabel var fluvax_mn \"Prc. Received Flu Vax.\"\nAnd now the code is the same as before.\ntwoway connected fluvax_mn year\ngraph export \"output\\fluvax_line1.png\", as(png) replace\n\n\n\nPercent of Americans receiving the Flu vaccine over time\n\n\n\n\n\nOften, we aren‚Äôt just interested in one group of people or one category of data at a time. Sometimes we want to monitor trends in multiple groups or one multiple measures at once. Let‚Äôs use the categorical measures of health as an example. For this, we have a little additional data cleaning setup to do, but the process is about the same - calculating the average of each variable of interest by year and then plotting the averages. Let‚Äôs get started with the data cleaning:\ntab health_nm, gen(healthcat)\negen hlth_excel_mn = mean(healthcat1), by(year)\negen hlth_vg_mn = mean(healthcat2), by(year)\negen hlth_g_mn = mean(healthcat3), by(year)\negen hlth_f_mn = mean(healthcat4), by(year)\negen hlth_p_mn = mean(healthcat5), by(year)\nAbove, we create an indicator variable for each category of the variable health named with a stub healthcat. This gives us five indicator variables. We then use those indicators to calculate the proportion of people in each category for each year in our sample.\n\n\n\n\n\n\nIndicator Variables\n\n\n\nRemember that indicator variables function as a true/false switch that takes a value of 1 if the observation falls into the category the variable is flagging and 0 if the observation does not. For instance, if I had 3 categories of education - high school or less, some college, and college or more - there would be three indicator variables (hs, somecol, col). For people with a high school diploma or less, the variable hs would be 1 and would be 0 for all others. For people who took some college classes but never got a 4-year degree, the variable somecol would be 1 and 0 for all others. And finally, people with a 4-year degree or more, the variable col would be 1 and 0 for all others. If we know everyone‚Äôs educational attainment, everyone will have exactly one of these variables coded as 1 and the others set to 0 in their row of our dataset. Since the average of a variable is \\(\\overline{X} = \\frac{\\sum x_i}{n}\\), the average of an indicator is the number of 1 values divided by the sample size. Or, put differently, the proportion of the sample in the indicator variable‚Äôs category!\n\n\nNow, let‚Äôs label these variables so that they make sense in our graph:\nlabel var hlth_excel_mn \"Prc. Excellent Health\"\nlabel var hlth_vg_mn \"Prc. Very Good Health\"\nlabel var hlth_g_mn \"Prc. Good Health\"\nlabel var hlth_f_mn \"Prc. Fair Health\"\nlabel var hlth_p_mn \"Prc. Poor Health\"\nAlright, now we are ready to graph this. The code for the graph is simple: we use the same command as before but we list all of our y-axis variables and then close with our x-axis variable.\ntwoway connected hlth_excel_mn hlth_vg_mn hlth_g_mn hlth_f_mn hlth_p_mn year\ngraph export \"output\\health_line1.png\", as(png) replace\nAnd we get a graph that looks like this:\n\n\n\nLine graph of trends of self-reported health status over time\n\n\nNow, because we based this on the means of indicators, our y-axis is on a 0 to 1 scale. Percents are, of course, on a 0-100 scale, and readers will find it mentally taxing to re-scale this in their minds themselves. So we might want to add an extra data cleaning step before we make our graph that scales the means appropriately because, as always, our job is to make reader‚Äôs lives easier.\ngen hlth_excel_mn100 = hlth_excel_mn*100\ngen hlth_vg_mn100 = hlth_vg_mn*100\ngen hlth_g_mn100 = hlth_g_mn*100\ngen hlth_f_mn100 = hlth_f_mn*100\ngen hlth_p_mn100 = hlth_p_mn*100\n\nlabel var hlth_excel_mn100 \"Prc. Excellent Health\"\nlabel var hlth_vg_mn100 \"Prc. Very Good Health\"\nlabel var hlth_g_mn100 \"Prc. Good Health\"\nlabel var hlth_f_mn100 \"Prc. Fair Health\"\nlabel var hlth_p_mn100 \"Prc. Poor Health\"\nNotice that the new variables simply multiply our original means by 100 (the * symbol in Stata is multiplication) to re-scale the variables to a 100 point scale. This will make our graph‚Äôs labels and range more intuitive to understand for most readers.\ntwoway connected hlth_excel_mn100 hlth_vg_mn100 hlth_g_mn100 hlth_f_mn100 hlth_p_mn100 year\ngraph export \"output\\health_line2.png\", as(png) replace\n\n\n\nRe-scaled line graph of trends of self-reported health status over time\n\n\nAnd that‚Äôs all for this lab! Let‚Äôs close our .do file with our usual log close and exit Stata.",
    "crumbs": [
      "Labs",
      "Lab 3 - Simple Visuals"
    ]
  },
  {
    "objectID": "probset2.html",
    "href": "probset2.html",
    "title": "Problem Set 2",
    "section": "",
    "text": "Begin by downloading the data you will need. We will again be using the Current Population Survey ASEC data like in problem set 1.\n\nGo to IPUMS.\nSelect only the ASEC samples from 2015-2019 and 2021-2024. You will need to de-select the basic monthly samples and should have 9 samples selected when finished.\nSelect the following variables:\n\n\nUnder Household -&gt; Core -&gt; Economic Characteristics, select HHINCOME, FOODSTMP, PUBHOUS.\nUnder Person -&gt; Core -&gt; Demographics, select AGE, SEX, MARST.\nUnder Person -&gt; Core -&gt; Work, select EMPSTAT, UHRSWORKT.\nUnder Person -&gt; Core -&gt; Education, select EDUC.\nUnder Person -&gt; Annual Social and Economic Supplement -&gt; Migration, select MIGRATE1.\n\nThis should leave you with an extract of 10 variables across 9 samples.\nIn this problem set, we are going to graph a few distributions of variables, look at a trend, and, for extra credit, make a few tables. But first, there are a few cleaning tasks you will need to do:\n\nCreate a new education variable, starting from EDUC, that uses four categories: Less than a high school diploma, a high school diploma, some college but less than 4-year degree, and 4-year college degree or more.\nCreate a binary variable for whether someone is employed, whether someone receives food stamps, whether someone is living in public housing, and whether someone moved to a new state in the past-year (MIGRATE1). That‚Äôs four new binary variables.\n\n\n\n\n\n\n\nHint\n\n\n\nWe have created versions of both the smaller education variable and the employment binary variable in previous labs. While the code might not be exactly the same for these variables - you‚Äôll want to check the categories and codes - the process for doing this is the same. You may also want to check the coding of the continuous variables to account for missings and topcodes properly.",
    "crumbs": [
      "Assignments",
      "Problem Set 2"
    ]
  },
  {
    "objectID": "probset2.html#extra-credit",
    "href": "probset2.html#extra-credit",
    "title": "Problem Set 2",
    "section": "Extra Credit",
    "text": "Extra Credit\n\nWhat is the difference in the average number of hours worked in a typical week between those in public housing and those not in public housing?\nWhat is the difference in the percentage of people who moved to a different state between those receiving food stamps and those not receiving food stamps?\nCreate a table with the distribution of educational attainment, average age, average household income, employment status, and average usual hours worked for the full sample, for those not in public housing, and for those in public housing.",
    "crumbs": [
      "Assignments",
      "Problem Set 2"
    ]
  },
  {
    "objectID": "probset3.html",
    "href": "probset3.html",
    "title": "Problem Set 3",
    "section": "",
    "text": "Instructions\nIn this week‚Äôs problem set, we are going to look at some health and demographic trends over the last several decades. We will be using this data - which I assembled for you already - from the National Health Interview Survey (NHIS) that includes samples from 1975 - 2022 in 5 year intervals (substituting the most recent post-COVID year for 2020). Some variables run the full year range, some do not. In the following questions, you will be asked to create some tables and graphs. This time, however, you are expected to use clean labels in both tables and graphs.\nAs always, you will be expected to report your answers in a word document and will submit the word document along with a .do file and .log file on Brightspace. The problem set is due before class next week.\n\n\nQuestions\n\nHow has the demographic make-up of the population and the workforce changed over time? Make a table that shows the distribution of race, average age, gender, high school diploma status, and 4-year college degree status for 1975, 1995, and 2015 using the full sample and then restricted to just employed people.\n\n\n\n\n\n\n\nHint\n\n\n\nThis table should have 6 columns. The first three should only be restricted by relevant years, the second three restricted by years and employed status.\n\n\n\nHow has primary cause of death changed over time? Create a line graph that plots primary cause of death for all years with available mortality data.\n\n\n\n\n\n\n\nHint 2\n\n\n\nThis should have 10 lines plotted.\n\n\n\nHow have smoking and drinking habits changed over time by poverty status? Create a line graph that plots the percent of people who report smoking 100 or more cigarettes in their lifetime and those who report drinking 12 or more alcoholic beverages per year by poverty status. Note, this means four lines.\nLet‚Äôs take a look at marital trends. Create a line graph showing the percent of people in each category of marital status over the time frame with marital status. Combine all married statuses into one married category for this purpose. Write one sentence describing a trend that jumps out to you.\nWe might be interested in whether there are differences in physical activity by poverty status and whether that has changed over time. Create a bar graph that presents the average number of times per week respondents engage in moderate physical activity for 10 or more minutes and vigorous physical activity for 10 or more minutes; do this over poverty status and by 2000 and 2015. What do you notice?\nCreate a horizontal bar graph that presents average moderate and vigorous activity per week across insurance status.\n\n\n\n\n\n\n\nHint 3\n\n\n\nThis will require you to create a new categorical variable that draws on all the existing information about insurance coverage that includes categories for no coverage and separate categories for each type of insurance we have data on. Consult the hhincome example from lab 1.\n\n\n\n\nExtra Credit\n\nCreate a line graph of the percent of people who report not having insurance coverage over the full time period for which we have coverage data.\nWhat is the difference in the percent of people below the poverty line with subsidized rent between 2000 and 2022?\nWhat is the difference in the percent of people below the poverty line without health insurance coverage between 1990 and 2015?\nLearn about SQL searches and then solve the murder mystery here. Who was the murderer? (Worth 2 points).",
    "crumbs": [
      "Assignments",
      "Problem Set 3"
    ]
  },
  {
    "objectID": "probset4.html",
    "href": "probset4.html",
    "title": "Problem Set 4 - EC",
    "section": "",
    "text": "Instructions\nIn this problem set, you will be using data from the Education Longitudinal Study of 2002 (ELS). The ELS contains data from a nationally representative sample of adolescents who were sophomores in high school in the base year of the study. The study checked in with them again in their senior year, the equivalent of their ‚Äúon-time‚Äù sophomore year of college, and again in 2012, when the college graduates in the sample would be early in their careers. In this set of problems, to help you prepare for the kinds of questions you will deal with in the midterm, you will be asked a few questions about proportions, distributions of categorical variables, means, and making a table and a bar graph. For these questions, aside from race, indicator variables have been created for all of the variables you need to answer the questions. When answering questions involving the race variable (byrace) or the educational attainment variable (f3attainment), you do not need to collapse the variables into fewer categories (though you may need to create clean, labeled indicator variables for each category in them). You can download the data here. There is a long-standing body of literature on inter-generational inequalities. Many of these questions will explore aspects of this using the ELS.\nThe assignment is entirely extra credit. Each question is worth 1 point. As always, submit a word document with your answers, your .do file, and your .log file.\n\n\nQuestions\n\nWhat is the average math and average reading score of students at urban high schools?\nWhat is the average math and average reading score of students at suburban high schools?\nLet‚Äôs think about exposure to diversity. The ELS asked the sophomores a few questions about their 3 closest friends at school. Create a horizontal bar graph that shows the average number of friends of a different racial background across categories of urbanicity.\nWhat is the average number of friends who care about their grades for people in the top quartile of math scores and people in the bottom quartile of math scores?\nNow let‚Äôs get a picture of characteristics by high- and low-income and by urbanicity. Create a table that shows the distribution of race and gender, average math and reading scores, average number of friends from a different race, average number of friends who believe grades are important, the proportion whose parents have a college degree and the proportion whose grandparents have a college degree. This table should present these statistics overall, separately by high- and low-income, and separately by urbanicity of the school.\n\n\n\n\n\n\n\nHint\n\n\n\nYour table should have 6 columns: 1 for everyone, 1 for low-income, 1 for high-income, and 1 for each of the 3 categories of urbanicity.\n\n\n\nIs educational attainment associated with parents‚Äô and grandparents‚Äô education? Create two horizontal bar graphs: i) show the student‚Äôs educational attainment by 2012 by parents with and without a college degree and ii) show the student‚Äôs educational attainment by 2012 by grandparents with and without a college degree.\nNow let‚Äôs look at schools. Principals were asked if various aspects of the school hindered learning at their school. Create a bar graph that shows the distribution of reasons learning is hindered at schools separately by high- and low-income students.\nLet‚Äôs get a sense for the school environment and resources at schools. Create a table that presents the average percent of kids in crisis programs, percent of kids in college preparatory programs, average number of guidance counselors, average teacher salaries, and the distribution of possible rewards for good teachers overall and separately by kids whose parents went to college and kids whose parents did not.\nCreate a table that shows the distribution of good teacher rewards separately by whether the principals believes teacher morale is high or not and whether the principal believes student morale is high or not.",
    "crumbs": [
      "Assignments",
      "Problem Set 4 - EC"
    ]
  },
  {
    "objectID": "group_assign.html",
    "href": "group_assign.html",
    "title": "Group Assignment",
    "section": "",
    "text": "To assess your understanding of the course material and your ability to apply concepts to real policy issues, you will be working in groups to prepare a 10 page report that i) describes a social problem with policy implications or an emerging policy issue, ii) uses data to describe the problem statistically, and iii) makes an informed recommendation on a policy change to improve the conditions that define the problem. The primary datasets used in the examples and problem sets throughout the course will be the datasets you will draw upon to construct your report. You will also prepare a presentation for these results to deliver at the end of the semester.\nYou and your group will choose one of the policy issues below as the focus of your report. Your analysis should be anchored in the background research on the policy area (the problem and the range of policy solutions that have been proposed/the effects of various attempts). After providing a background on the debates about the policy problem and evidence about both direct and indirect effects of the problem and the policies tackling the problem, you will use data from IPUMS or other official sources to prepare at least two tables and two graphs that describe the problem.\n\n\n\n\nRising college costs has been an important contemporary issue affecting both lasting indebtedness and the accessibility of college for some segments in the population. As the returns to college degrees grew over time, the affordability of college has become a dimension of economic inequality as well. In 2017, the Cuomo administration passed the Excelsior Scholarship, expanding tuition-free public college options for middle class families making up to $125,000 per year in New York. Has the program changed college enrollment patterns in New York state? If so, how?\n\n\n\n\n\n\nNote\n\n\n\nThe ACS and CPS both collect information from respondents about their college enrollment status, alongside the demographic and socioeconomic status details of their families. In addition, you can make use of the Common Core of Data or IPEDS to get a sense for college enrollment demographics and get a rough comparison of 12th grade enrollments and college fall semester enrollments.\n\n\n\n\n\nEarly in the Biden Administration, bills were considered for making permanent the expanded child tax credit and for expanding pre-Kindergarten services for all families.1 Does access to child care improve the employment rate among parents of young children? What does the research say about the effects of access to pre-K on children and their parents? What areas of the country and types of households would have been most impacted by a universal pre-K program? In the past, have people in counties with more access to child care subsidies tended to have more children? Are there other impacts of unmet child care needs?\n\n\n\n\n\n\nNote\n\n\n\nThe CPS collects information about respondent‚Äôs childcare needs and the ASEC collects information about the dollar amount of the child tax credit received by families and money spent on child care among respondents in the poverty supplement sample.\n\n\n\n\n\nBeyond CO2 emissions, combustion vehicles create a wide array of emissions that can negatively impact the environment. What does the research say about the effects of transportation emissions on health, well-being, and environmental outcomes in communities? Are there any other effects of transportation emissions? Using data on transit availability and reported commuting mode, describe the impact of the transportation mix of commuters on the average monthly AQI in counties. In addition, you should examine how health outcomes vary by transit mix and AQI.\n\n\n\n\n\n\nNote\n\n\n\nThe ACS, CPS, and ATUS all contain information about commuting and can be linked to the most populous counties.\n\n\n\n\n\n\nYour papers should be organized into 5 main parts: A 1-page executive summary that summarizes the issue and the main findings from your report (this does not count toward the 10 pages for the paper), an introduction that provides an outline of the paper and preview of the take-aways, a background section that provides and overview of the existing research on the issue, your analysis section, and a concluding discussion.",
    "crumbs": [
      "Assignments",
      "Group Assignment"
    ]
  },
  {
    "objectID": "group_assign.html#policy-issues",
    "href": "group_assign.html#policy-issues",
    "title": "Group Assignment",
    "section": "",
    "text": "Rising college costs has been an important contemporary issue affecting both lasting indebtedness and the accessibility of college for some segments in the population. As the returns to college degrees grew over time, the affordability of college has become a dimension of economic inequality as well. In 2017, the Cuomo administration passed the Excelsior Scholarship, expanding tuition-free public college options for middle class families making up to $125,000 per year in New York. Has the program changed college enrollment patterns in New York state? If so, how?\n\n\n\n\n\n\nNote\n\n\n\nThe ACS and CPS both collect information from respondents about their college enrollment status, alongside the demographic and socioeconomic status details of their families. In addition, you can make use of the Common Core of Data or IPEDS to get a sense for college enrollment demographics and get a rough comparison of 12th grade enrollments and college fall semester enrollments.\n\n\n\n\n\nEarly in the Biden Administration, bills were considered for making permanent the expanded child tax credit and for expanding pre-Kindergarten services for all families.1 Does access to child care improve the employment rate among parents of young children? What does the research say about the effects of access to pre-K on children and their parents? What areas of the country and types of households would have been most impacted by a universal pre-K program? In the past, have people in counties with more access to child care subsidies tended to have more children? Are there other impacts of unmet child care needs?\n\n\n\n\n\n\nNote\n\n\n\nThe CPS collects information about respondent‚Äôs childcare needs and the ASEC collects information about the dollar amount of the child tax credit received by families and money spent on child care among respondents in the poverty supplement sample.\n\n\n\n\n\nBeyond CO2 emissions, combustion vehicles create a wide array of emissions that can negatively impact the environment. What does the research say about the effects of transportation emissions on health, well-being, and environmental outcomes in communities? Are there any other effects of transportation emissions? Using data on transit availability and reported commuting mode, describe the impact of the transportation mix of commuters on the average monthly AQI in counties. In addition, you should examine how health outcomes vary by transit mix and AQI.\n\n\n\n\n\n\nNote\n\n\n\nThe ACS, CPS, and ATUS all contain information about commuting and can be linked to the most populous counties.",
    "crumbs": [
      "Assignments",
      "Group Assignment"
    ]
  },
  {
    "objectID": "group_assign.html#footnotes",
    "href": "group_assign.html#footnotes",
    "title": "Group Assignment",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nCurrently, the federal government provides pre-Kindergarten services for low-income families through Head Start programs.‚Ü©Ô∏é",
    "crumbs": [
      "Assignments",
      "Group Assignment"
    ]
  },
  {
    "objectID": "group_assign.html#structure",
    "href": "group_assign.html#structure",
    "title": "Group Assignment",
    "section": "",
    "text": "Your papers should be organized into 5 main parts: A 1-page executive summary that summarizes the issue and the main findings from your report (this does not count toward the 10 pages for the paper), an introduction that provides an outline of the paper and preview of the take-aways, a background section that provides and overview of the existing research on the issue, your analysis section, and a concluding discussion.",
    "crumbs": [
      "Assignments",
      "Group Assignment"
    ]
  },
  {
    "objectID": "lab5.html",
    "href": "lab5.html",
    "title": "Lab 5 - Making Maps",
    "section": "",
    "text": "Making maps can be an incredibly useful and powerful analytic and descriptive tool in any analyst‚Äôs toolkit, and that‚Äôs particularly true for anyone working around public policy. Maps can help us visualize a problem, spot particularly troublesome areas in the geographic space we are governing, and identify areas that might need new or different allocations of resources. In this lab, we are going to make a few different maps using data from the Common Core of Data (CCD) - the national data warehouse for data on public schools - and the American Time Use Survey (ATUS). I have already downloaded and cleaned the data we will need here, but I will walk through some of the cleaning in the setup so that you can re-create the process from scratch.\n\n\n\n\n\n\nWarning\n\n\n\nGeographic Information System (GIS) data is a very broad field. This lab is a short introduction to some basic concepts, but the topic could fit an entire semester of content. Broadly, you can think of two buckets of technical skills important for working with GIS applications - production of GIS data and use of GIS data. We will be working in the latter bucket of skills here.\n\n\n\n\nGIS data refers to data that has been collected and stored in a way that can allow for the projection of data onto a geographic representation of where that data was produced in the physical world. The primary software and service used in professional applications of GIS data is ArcGIS or a combination of packages in R that often works with ArcGIS. However, Stata has several packages and integrations that also allow for the creation of fairly polished maps with data. To make use of GIS data, you often need data that has been geocoded - that is, observations also include longitudinal and latitudinal coordinates for where an observation took place and, often, x and y coordinates for mapping the latitude and longitude onto a given map shape. To work in Stata, you are generally going to need a few things:\n\nA shapefile (typically in the ESRI format).\nA dataset with geographic identifiers.\nA dataset with both identifiers and the information you want to graph on a map.\n\nThe Census Bureau has created a wealth of ESRI format shapefiles for many geogrphic units in the United States. Often a shapefile you need can be found using a simple search for what you‚Äôre trying to map (e.g., New York) and shapefile, e.g., ‚ÄúNew York shapefile.‚Äù For instance, New York‚Äôs can be found here. Downloading the .zip folder with the shapefile will reveal a whole host of files:\n\nWe only need the .dbf and the .shp file for working with the New York map in Stata. Don‚Äôt worry: we won‚Äôt be working with the complexities of this geometric data directly. We will use some packages that will convert this data into simple geographic identifiers we can use to merge our geocoded data into and use Stata to create the maps we want.\nThe basic process for making a map will be: 1. Download the shapefile and the data you want to map. 2. Ensure you have a matching geographic identifier in both the shapefile and the geocoded data you want to map. 3. Convert the shapefile to a Stata .dta datafile. 4. Merge in the data you want to map. 5. Use spmap or maptile to make the map you need.\n\n\n\nFor the CCD school data, there were two datafiles used to create the data we will be using in this lab. We will begin by creating a map of all the public schools in New York state. We will then look at a few ways to modify the look and feel of that map. And we will close by creating a map that plots schools by the size of their student body. What this means is we need 1) the directory data of the schools with geocodes and 2) school-level enrollment data. NCES provides the addresses for all public schools in the US in their directory data and, through a collaboration with ArcGIS, the geocoded directory data is publicly available (the raw datafiles can be downloaded here). We will also want enrollment data for the last step, so I downloaded the membership data (found here), cleaned it, and merged it with the geocoded directory data to create the data for this lab.\n\n\n\n\n\n\nNote\n\n\n\nI walk through the cleaning of this data at the end of the lab.\n\n\nThe geocoded data looks like this:\n\n\n\nSnippet of geocoded data\n\n\nI want to flag a few important features of the data that makes this data useful. First, included in the data are the x and y coordinates of the location of the school (named, helpfully, x and y). You‚Äôll notice that these are the same as the longitude and latitude coordinates of the address. The data also provides the street address, which can be used with other packages to get geographic coordinates if you are working with data that does not have the coordinates already coded for you. Second, the data includes a unique identifier for every school (ncessch) that allows us to link this geographic information with other school characteristics. This can be useful for visualizing things like schools with concentrated poverty, areas with highly racially segregated schools, areas with a lot of highly effective schools1, and many other policy relevant data elements. Finally, not shown in the snippet but in the data, you can find multiple codes for different levels of geographic information - counties, MSAs, states, and so on - that can be used to link the school data to other aspects of the communities shools are in. For our purposes, we will be using cnty (below) to link this data with the shapefile data for our maps. Note that the cnty variable in our schools data is a numeric variable and already includes the state fips (36) and county fips (the last three digits). I flag this here because we will need a matching identifier in our shapefile data (this will make sense later).\n\n\n\n\nIn Stata, there are a few different packages that assist in making maps. A lot of the packages are for helping Stata convert the file formats common in GIS datasets into a format Stata can recognize and use. The rest are packages that provide syntax for creating maps with Stata code - important for our purposes of using GIS data. The two main packages for making maps in Stata are spmap and maptile. While spmap is more flexible and allows you to use your own map inputs for making more custom maps, that flexibility comes at the cost of being a little more complex to use well. Meanwhile, maptile is a simpler wrapper for spmap that allows you to plot quantiles of data across counties or states in a variety of styles; it is less flexible but far more straightforward to use. We will go through some examples using both.2\n\n\nLet‚Äôs begin by starting our .do file and installing the packages we will need.\ncd \"C:\\rpad504\\\"\nlocal today : di %tdCY.N.D date(\"$S_DATE\",\"DMY\")\nlog using \"logs\\lab5_`today'.log\", replace\n\nssc install spmap, replace\nssc install shp2dta, replace\nssc install mif2dta, replace\nssc install maptile, replace\nssc install statastates, replace\nThere are also a few different map templates available in maptile. We will go ahead and download those too.\nmaptile_install using \"http://files.michaelstepner.com/geo_county2014.zip\", replace\nmaptile_install using \"http://files.michaelstepner.com/geo_state.zip\", replace\nmaptile_install using \"http://files.michaelstepner.com/geo_statehex.zip\", replace\nAlright, now we have everything we need to make some maps. Let‚Äôs get started.\n\n\n\nWe will start by creating a few maps of all the public schools in New York. To do this, we will be using spmap and the ny_schools_geo.dta datafile in the lab folder. We will start by using the shp2dta command we just installed to create a dataset with the geographic identifiers for merging in data and a dataset with the coordinates for making the map. The code is fairly straightforward.\nshp2dta using \"data\\tl_2016_36_cousub.shp\", database(nydb) coordinates(nycoord)\nNote that nydb and nycoord are simply names I chose for the datafiles with geographic identifiers and coordinates, respectively. The command will generate two new .dta files in your main folder. Let‚Äôs open the nydb dataset first and look at the geographic identifiers.\nuse \"nydb.dta\"\n\nNote that the state FIPS and county FIPS are separate in this datafile. They are also yellow, which tells us that they are stored as a string format in Stata rather than a number. We will need to create a new variable with the same name as the county identifier in our school data (cnty) and the same format (numeric). We will do that with two simple lines: concatenate (which serves to combine the information from two variables into a single variable, preserving the order of the digits in the new combined variable) and generating a numeric version of the string variable. Then we will save and clear this data.\negen cnty1 = concat(STATEFP COUNTYFP)\ngen cnty = real(cnty1)\nsort cnty\nsave \"data\\nydb.dta\", replace\nclear\nNow let‚Äôs open the schools data and merge in the geoidentifers from the shapefile using the cnty variable we just created.\nuse \"data\\ny_schools_geo.dta\"\nsort cnty\nmerge m:m cnty using \"data\\nydb.dta\"\ndrop _merge\negen schid = group(name street ncessch)\nsort schid\nquietly by schid: gen dup = cond(_N==1,0,_n)\ntab dup\ndrop if dup&gt;0\nAfter we remove duplicates in our datafile, we should have 4,800 observations. Now we‚Äôre ready to start mapping! We will use the unique school identifier we created in our previous step (grouping the observations by name, street, and ncessch or the school name, street address, and NCES identifier).\n\n\n\n\n\n\nNote\n\n\n\nThis is not an ideal data structure because we will be relying on the awful m:m merge that we should avoid in nearly all cases. A better shapefile would have been one using New York school districts as the organizing unit. However, our county map will suffice for illistrating map creation even in suboptimal circumstances.\n\n\nLet‚Äôs start with a very basic map of all the schools in New York.\nspmap using nycoord, id(schid) point(xcoord(x) ycoord(y))\ngraph export \"output\\nymap1.png\", replace\nWhich should give us something like this:\n\n\n\nNew York Schools\n\n\nOf course, these dots are a bit large and somewhat hard to see. Let‚Äôs go ahead and change their color to something that might stand out a bit more.\nspmap using nycoord, id(schid) point(xcoord(x) ycoord(y) fcolor(blue))\ngraph export \"output\\nymap2.png\", replace\n\n\n\nNew York Schools Again!\n\n\nLet‚Äôs try a different plot style.\nspmap using nycoord, id(schid) point(xcoord(x) ycoord(y) fcolor(blue) shape(+))\ngraph export \"output\\nymap3.png\", replace\n\n\n\nNew York Schools: Return of the Schools\n\n\nNote that the center of the plus signs is the more precise location of the school. Okay, so that‚Äôs how you change the style and the color of the symbols on the map. You can add a legend to capture what symbols mean and plot different types of schools (say, charter versus traditional or elementary, middle, and high schools) with different symbols (using the by option). The help file for spmap provides a comprehensive run-down of those options. For now, we are going to add a little more information to our map. Schools vary quite considerably in their size and it would be great if we could visualize where the large schools are in the state. Well, since I cleaned and merged the enrollment data into our schools dataset, we can use the enrollment count at the schools to inform the relative size of our school plot points. Specifically, this will have Stata create plots of varying sizes where the size of the plot captures the number of students in a given school relative to other schools in the dataset (in this case, the state).\nspmap using nycoord, id(schid) point(xcoord(x) ycoord(y) fcolor(blue) shape(o) prop(enroll))\ngraph export \"output\\nymap4.png\", replace\n\n\n\nNew York Schools: The Schools Awaken\n\n\nNotice that this creates kind of an ugly blob of plots. Yes, some of them vary in size, but it‚Äôs really hard to tell what‚Äôs going here. We‚Äôre going to add and outline to the plot points and tone down the brightness of the fill color to get a better sense of the data.\nspmap using nycoord, id(schid) point(xcoord(x) ycoord(y) fcolor(blue*0.25) ocolor(black) shape(o) prop(enroll))\ngraph export \"output\\nymap5.png\", replace\n\n\n\nNew York Schools: The Last School Map\n\n\nMuch better! Note that this sort of map, in conjunction with good, clean, well organized data, can provide similar information about other kinds of variables (e.g., caseload across welfare or drug addiction treatment offices, homeless services, crime hotspots by type of crime). The maps created here are just a rough guide to help you understand how to make maps with data in Stata to get you started.\n\n\n\nOf course, we often want to know how things look across the country and how states are doing relative to one another. For this kind of analysis, we might find the simplicity of maptile to be more flexible. Let‚Äôs clear out the data in our memory and load the volunteering data:\nclear\nuse \"data\\state_volunteer.dta\"\nThis dataset has been cleaned and includes state names, state abbreviations, state FIPS codes, and the average minutes spent volunteering in a typical data drawing on ATUS data from 2003 to 2017. Since I have already setup the data at the state level, we can dive right in to making maps with maptile. Let‚Äôs start with a conventional map of states:\nmaptile state_volunteer, geo(state)\ngraph export \"output\\volmap1.png\", replace\nYielding this map:\n\n\n\nVolunteering in the US\n\n\nIn maptile syntax, the geo option specifies which geographic map you are using. Note that you have to download the geographic map you want to use ahead of time (we did this at the beginning of the lab). You can find a variety of compatible maps on the creator‚Äôs website. What maptile does is it takes the variable you specifie (in our case state_volunteer) and bins it into quantiles of that variable and then shades the states according to their quantile. You can specify the number of quantiles you‚Äôd like to use or the range of each bin you want to group observations by in place of quantiles and some types of data may call for these modifications.\nNote that in a conventional map, like we see above, the variation in state geographic sizes might hide some states (my beloved D.C.!) or misrepresent the prevalence of something (e.g., looking at vote shares by party ID might be misleading at the state-level because some states have more sparsely populated land - here‚Äôs looking at you Wyoming). One approach to fix this favored by some is to use evenly sized hexagons for presenting state data. Here, since we downloaded hex maps earlier, we would simply change the geographic code and point to the variable that has abbreviations for labeling the hexagons (important now that they are losing their distinctive shapes).\nmaptile state_volunteer, geo(statehex) labelhex(state)\ngraph export \"output\\volmap2.png\", replace\n\n\n\nVolunteering in the US, put differently\n\n\nAnd that‚Äôs all for this lab on mapping. Let‚Äôs end our log.\nlog close",
    "crumbs": [
      "Labs",
      "Lab 5 - Making Maps"
    ]
  },
  {
    "objectID": "lab5.html#gis-data",
    "href": "lab5.html#gis-data",
    "title": "Lab 5 - Making Maps",
    "section": "",
    "text": "GIS data refers to data that has been collected and stored in a way that can allow for the projection of data onto a geographic representation of where that data was produced in the physical world. The primary software and service used in professional applications of GIS data is ArcGIS or a combination of packages in R that often works with ArcGIS. However, Stata has several packages and integrations that also allow for the creation of fairly polished maps with data. To make use of GIS data, you often need data that has been geocoded - that is, observations also include longitudinal and latitudinal coordinates for where an observation took place and, often, x and y coordinates for mapping the latitude and longitude onto a given map shape. To work in Stata, you are generally going to need a few things:\n\nA shapefile (typically in the ESRI format).\nA dataset with geographic identifiers.\nA dataset with both identifiers and the information you want to graph on a map.\n\nThe Census Bureau has created a wealth of ESRI format shapefiles for many geogrphic units in the United States. Often a shapefile you need can be found using a simple search for what you‚Äôre trying to map (e.g., New York) and shapefile, e.g., ‚ÄúNew York shapefile.‚Äù For instance, New York‚Äôs can be found here. Downloading the .zip folder with the shapefile will reveal a whole host of files:\n\nWe only need the .dbf and the .shp file for working with the New York map in Stata. Don‚Äôt worry: we won‚Äôt be working with the complexities of this geometric data directly. We will use some packages that will convert this data into simple geographic identifiers we can use to merge our geocoded data into and use Stata to create the maps we want.\nThe basic process for making a map will be: 1. Download the shapefile and the data you want to map. 2. Ensure you have a matching geographic identifier in both the shapefile and the geocoded data you want to map. 3. Convert the shapefile to a Stata .dta datafile. 4. Merge in the data you want to map. 5. Use spmap or maptile to make the map you need.",
    "crumbs": [
      "Labs",
      "Lab 5 - Making Maps"
    ]
  },
  {
    "objectID": "lab5.html#data-for-this-lab",
    "href": "lab5.html#data-for-this-lab",
    "title": "Lab 5 - Making Maps",
    "section": "",
    "text": "For the CCD school data, there were two datafiles used to create the data we will be using in this lab. We will begin by creating a map of all the public schools in New York state. We will then look at a few ways to modify the look and feel of that map. And we will close by creating a map that plots schools by the size of their student body. What this means is we need 1) the directory data of the schools with geocodes and 2) school-level enrollment data. NCES provides the addresses for all public schools in the US in their directory data and, through a collaboration with ArcGIS, the geocoded directory data is publicly available (the raw datafiles can be downloaded here). We will also want enrollment data for the last step, so I downloaded the membership data (found here), cleaned it, and merged it with the geocoded directory data to create the data for this lab.\n\n\n\n\n\n\nNote\n\n\n\nI walk through the cleaning of this data at the end of the lab.\n\n\nThe geocoded data looks like this:\n\n\n\nSnippet of geocoded data\n\n\nI want to flag a few important features of the data that makes this data useful. First, included in the data are the x and y coordinates of the location of the school (named, helpfully, x and y). You‚Äôll notice that these are the same as the longitude and latitude coordinates of the address. The data also provides the street address, which can be used with other packages to get geographic coordinates if you are working with data that does not have the coordinates already coded for you. Second, the data includes a unique identifier for every school (ncessch) that allows us to link this geographic information with other school characteristics. This can be useful for visualizing things like schools with concentrated poverty, areas with highly racially segregated schools, areas with a lot of highly effective schools1, and many other policy relevant data elements. Finally, not shown in the snippet but in the data, you can find multiple codes for different levels of geographic information - counties, MSAs, states, and so on - that can be used to link the school data to other aspects of the communities shools are in. For our purposes, we will be using cnty (below) to link this data with the shapefile data for our maps. Note that the cnty variable in our schools data is a numeric variable and already includes the state fips (36) and county fips (the last three digits). I flag this here because we will need a matching identifier in our shapefile data (this will make sense later).",
    "crumbs": [
      "Labs",
      "Lab 5 - Making Maps"
    ]
  },
  {
    "objectID": "lab5.html#footnotes",
    "href": "lab5.html#footnotes",
    "title": "Lab 5 - Making Maps",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nDefining and identifying effective schools is a large area of research with several active debates. One useful and fairly robust measure relies on school-specific growth scores, but that‚Äôs a topic for another class.‚Ü©Ô∏é\nThese are both user-written packages by Michael Stepner at MIT (maptile) and Maurizio Pisati (spmap). User contributions to both Stata and R are the difficult, voluntary, but wildly valuable work that some scholars and data analysts do to make these tools widely available to analysts and researchers everywhere.‚Ü©Ô∏é",
    "crumbs": [
      "Labs",
      "Lab 5 - Making Maps"
    ]
  },
  {
    "objectID": "lab5.html#making-maps-in-stata",
    "href": "lab5.html#making-maps-in-stata",
    "title": "Lab 5 - Making Maps",
    "section": "",
    "text": "In Stata, there are a few different packages that assist in making maps. A lot of the packages are for helping Stata convert the file formats common in GIS datasets into a format Stata can recognize and use. The rest are packages that provide syntax for creating maps with Stata code - important for our purposes of using GIS data. The two main packages for making maps in Stata are spmap and maptile. While spmap is more flexible and allows you to use your own map inputs for making more custom maps, that flexibility comes at the cost of being a little more complex to use well. Meanwhile, maptile is a simpler wrapper for spmap that allows you to plot quantiles of data across counties or states in a variety of styles; it is less flexible but far more straightforward to use. We will go through some examples using both.2\n\n\nLet‚Äôs begin by starting our .do file and installing the packages we will need.\ncd \"C:\\rpad504\\\"\nlocal today : di %tdCY.N.D date(\"$S_DATE\",\"DMY\")\nlog using \"logs\\lab5_`today'.log\", replace\n\nssc install spmap, replace\nssc install shp2dta, replace\nssc install mif2dta, replace\nssc install maptile, replace\nssc install statastates, replace\nThere are also a few different map templates available in maptile. We will go ahead and download those too.\nmaptile_install using \"http://files.michaelstepner.com/geo_county2014.zip\", replace\nmaptile_install using \"http://files.michaelstepner.com/geo_state.zip\", replace\nmaptile_install using \"http://files.michaelstepner.com/geo_statehex.zip\", replace\nAlright, now we have everything we need to make some maps. Let‚Äôs get started.\n\n\n\nWe will start by creating a few maps of all the public schools in New York. To do this, we will be using spmap and the ny_schools_geo.dta datafile in the lab folder. We will start by using the shp2dta command we just installed to create a dataset with the geographic identifiers for merging in data and a dataset with the coordinates for making the map. The code is fairly straightforward.\nshp2dta using \"data\\tl_2016_36_cousub.shp\", database(nydb) coordinates(nycoord)\nNote that nydb and nycoord are simply names I chose for the datafiles with geographic identifiers and coordinates, respectively. The command will generate two new .dta files in your main folder. Let‚Äôs open the nydb dataset first and look at the geographic identifiers.\nuse \"nydb.dta\"\n\nNote that the state FIPS and county FIPS are separate in this datafile. They are also yellow, which tells us that they are stored as a string format in Stata rather than a number. We will need to create a new variable with the same name as the county identifier in our school data (cnty) and the same format (numeric). We will do that with two simple lines: concatenate (which serves to combine the information from two variables into a single variable, preserving the order of the digits in the new combined variable) and generating a numeric version of the string variable. Then we will save and clear this data.\negen cnty1 = concat(STATEFP COUNTYFP)\ngen cnty = real(cnty1)\nsort cnty\nsave \"data\\nydb.dta\", replace\nclear\nNow let‚Äôs open the schools data and merge in the geoidentifers from the shapefile using the cnty variable we just created.\nuse \"data\\ny_schools_geo.dta\"\nsort cnty\nmerge m:m cnty using \"data\\nydb.dta\"\ndrop _merge\negen schid = group(name street ncessch)\nsort schid\nquietly by schid: gen dup = cond(_N==1,0,_n)\ntab dup\ndrop if dup&gt;0\nAfter we remove duplicates in our datafile, we should have 4,800 observations. Now we‚Äôre ready to start mapping! We will use the unique school identifier we created in our previous step (grouping the observations by name, street, and ncessch or the school name, street address, and NCES identifier).\n\n\n\n\n\n\nNote\n\n\n\nThis is not an ideal data structure because we will be relying on the awful m:m merge that we should avoid in nearly all cases. A better shapefile would have been one using New York school districts as the organizing unit. However, our county map will suffice for illistrating map creation even in suboptimal circumstances.\n\n\nLet‚Äôs start with a very basic map of all the schools in New York.\nspmap using nycoord, id(schid) point(xcoord(x) ycoord(y))\ngraph export \"output\\nymap1.png\", replace\nWhich should give us something like this:\n\n\n\nNew York Schools\n\n\nOf course, these dots are a bit large and somewhat hard to see. Let‚Äôs go ahead and change their color to something that might stand out a bit more.\nspmap using nycoord, id(schid) point(xcoord(x) ycoord(y) fcolor(blue))\ngraph export \"output\\nymap2.png\", replace\n\n\n\nNew York Schools Again!\n\n\nLet‚Äôs try a different plot style.\nspmap using nycoord, id(schid) point(xcoord(x) ycoord(y) fcolor(blue) shape(+))\ngraph export \"output\\nymap3.png\", replace\n\n\n\nNew York Schools: Return of the Schools\n\n\nNote that the center of the plus signs is the more precise location of the school. Okay, so that‚Äôs how you change the style and the color of the symbols on the map. You can add a legend to capture what symbols mean and plot different types of schools (say, charter versus traditional or elementary, middle, and high schools) with different symbols (using the by option). The help file for spmap provides a comprehensive run-down of those options. For now, we are going to add a little more information to our map. Schools vary quite considerably in their size and it would be great if we could visualize where the large schools are in the state. Well, since I cleaned and merged the enrollment data into our schools dataset, we can use the enrollment count at the schools to inform the relative size of our school plot points. Specifically, this will have Stata create plots of varying sizes where the size of the plot captures the number of students in a given school relative to other schools in the dataset (in this case, the state).\nspmap using nycoord, id(schid) point(xcoord(x) ycoord(y) fcolor(blue) shape(o) prop(enroll))\ngraph export \"output\\nymap4.png\", replace\n\n\n\nNew York Schools: The Schools Awaken\n\n\nNotice that this creates kind of an ugly blob of plots. Yes, some of them vary in size, but it‚Äôs really hard to tell what‚Äôs going here. We‚Äôre going to add and outline to the plot points and tone down the brightness of the fill color to get a better sense of the data.\nspmap using nycoord, id(schid) point(xcoord(x) ycoord(y) fcolor(blue*0.25) ocolor(black) shape(o) prop(enroll))\ngraph export \"output\\nymap5.png\", replace\n\n\n\nNew York Schools: The Last School Map\n\n\nMuch better! Note that this sort of map, in conjunction with good, clean, well organized data, can provide similar information about other kinds of variables (e.g., caseload across welfare or drug addiction treatment offices, homeless services, crime hotspots by type of crime). The maps created here are just a rough guide to help you understand how to make maps with data in Stata to get you started.\n\n\n\nOf course, we often want to know how things look across the country and how states are doing relative to one another. For this kind of analysis, we might find the simplicity of maptile to be more flexible. Let‚Äôs clear out the data in our memory and load the volunteering data:\nclear\nuse \"data\\state_volunteer.dta\"\nThis dataset has been cleaned and includes state names, state abbreviations, state FIPS codes, and the average minutes spent volunteering in a typical data drawing on ATUS data from 2003 to 2017. Since I have already setup the data at the state level, we can dive right in to making maps with maptile. Let‚Äôs start with a conventional map of states:\nmaptile state_volunteer, geo(state)\ngraph export \"output\\volmap1.png\", replace\nYielding this map:\n\n\n\nVolunteering in the US\n\n\nIn maptile syntax, the geo option specifies which geographic map you are using. Note that you have to download the geographic map you want to use ahead of time (we did this at the beginning of the lab). You can find a variety of compatible maps on the creator‚Äôs website. What maptile does is it takes the variable you specifie (in our case state_volunteer) and bins it into quantiles of that variable and then shades the states according to their quantile. You can specify the number of quantiles you‚Äôd like to use or the range of each bin you want to group observations by in place of quantiles and some types of data may call for these modifications.\nNote that in a conventional map, like we see above, the variation in state geographic sizes might hide some states (my beloved D.C.!) or misrepresent the prevalence of something (e.g., looking at vote shares by party ID might be misleading at the state-level because some states have more sparsely populated land - here‚Äôs looking at you Wyoming). One approach to fix this favored by some is to use evenly sized hexagons for presenting state data. Here, since we downloaded hex maps earlier, we would simply change the geographic code and point to the variable that has abbreviations for labeling the hexagons (important now that they are losing their distinctive shapes).\nmaptile state_volunteer, geo(statehex) labelhex(state)\ngraph export \"output\\volmap2.png\", replace\n\n\n\nVolunteering in the US, put differently\n\n\nAnd that‚Äôs all for this lab on mapping. Let‚Äôs end our log.\nlog close",
    "crumbs": [
      "Labs",
      "Lab 5 - Making Maps"
    ]
  },
  {
    "objectID": "probset5.html",
    "href": "probset5.html",
    "title": "Problem Set 5",
    "section": "",
    "text": "Introduction\nIn this problem set, you will use data from the American Time Use Survey (ATUS) to create maps at the state and county level of time spent on social and recreational activities, time spent on education-related activities, and time spent working. All data have been aggregated to the state-level for you and are measured in minutes spent on an average day. The data can be downloaded here. You will create the maps asked for in the question and write no more than three sentences using the maps to answer the question. Turn in a word document with your maps and answers, a .do file with your code, and a .log file with your log files from your analysis.\n\n\n\n\n\n\nNote\n\n\n\nQuestions 1 and 2 will use maptile while question 3 will use spmap. In the ATUS, only counties with a population greater than 100,000 people are included.\n\n\n\n\nQuestions\n\nCreate a state map that shows time spent on education-related activities in 2005, 2019, and 2023 (3 separate maps). How have the patterns of time spent on education activities across states changed over time? Which states have changed the most?\n\n\n\n\n\n\n\nNote\n\n\n\nThese can use either real state boundaries or state hexes.\n\n\n\nCreate a map that presents time spent on work-related acitivities and socializing and recreational activities across counties (2 separate maps). What jumps out to you when comparing counties on these two time-use dimensions?\nUsing the shape file and the ny_cleanupsites.dta data, create a map that plots the environmental clean-up sites across New York. Represent the sites as blue plus signs.\n\n\n\n\n\n\n\nTip\n\n\n\nYou will want to use a m:m merge on county, like we did in the lab, but may want to create a lowercase version of county before you do so. You will want to use the force option with your merge command and only keep observations where _merge == 3 after the merge, for the purposes of this exercise. Note that objectid functions the same as schid in this context.\n\n\n\n\nExtra Credit\n\nRecreate this map and type the code you used underneath your map. Worth 5 points.\n\n\n\n\n\n\n\nHint\n\n\n\nThe color scheme used in the map is Dark2. The rest uses customization of the map made for question 3 using a few options you can find in the help file.\n\n\n\n\n\nCleanup Sites in New York",
    "crumbs": [
      "Assignments",
      "Problem Set 5"
    ]
  }
]